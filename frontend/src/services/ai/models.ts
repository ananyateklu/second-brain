import { AIModel } from '../../types/ai';

export const AI_MODELS: AIModel[] = [
  // OpenAI GPT-4 Models
  {
    id: 'o1-preview',
    name: 'O1 Preview',
    provider: 'openai',
    category: 'chat',
    description: 'Most capable reasoning model',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'chat',
    rateLimits: {
      tpm: 8000,
      rpm: 500,
      maxOutputTokens: 8192,
      maxInputTokens: 10000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-4-turbo',
    name: 'GPT-4 Turbo',
    provider: 'openai',
    category: 'chat',
    description: 'Enhanced GPT-4 with higher rate limits',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'chat',
    rateLimits: {
      tpm: 30000,
      rpm: 500,
      tpd: 90000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-4o',
    name: 'GPT-4 Optimized',
    provider: 'openai',
    category: 'chat',
    description: 'Optimized version of GPT-4',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'chat',
    rateLimits: {
      tpm: 30000,
      rpm: 500,
      tpd: 90000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-4o-mini',
    name: 'GPT-4 Optimized Mini',
    provider: 'openai',
    category: 'chat',
    description: 'Lightweight GPT-4 with higher token limits',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'chat',
    rateLimits: {
      tpm: 200000,
      rpm: 500,
      rpd: 10000,
      tpd: 2000000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-3.5-turbo',
    name: 'GPT-3.5 Turbo',
    provider: 'openai',
    category: 'chat',
    description: 'Fast and cost-effective chat model',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'chat',
    rateLimits: {
      tpm: 200000,
      rpm: 500,
      rpd: 10000,
      tpd: 2000000,
    },
    size: '175B',
  },
  // OpenAI Embedding Models
  {
    id: 'text-embedding-3-small',
    name: 'Text Embedding 3 Small',
    provider: 'openai',
    category: 'embedding',
    description: 'Efficient text embedding generation',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'embeddings',
    rateLimits: {
      tpm: 1000000,
      rpm: 3000,
      tpd: 3000000,
    },
    size: '1.5B',
  },
  // OpenAI Image Models
  {
    id: 'dall-e-3',
    name: 'DALL·E 3',
    provider: 'openai',
    category: 'image',
    description: 'Advanced image generation',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'images',
    rateLimits: {
      rpm: 500,
      imagesPerMinute: 5,
    },
    size: '20B',
  },
  // OpenAI Audio Models
  {
    id: 'whisper-1',
    name: 'Whisper v1',
    provider: 'openai',
    category: 'audio',
    description: 'Speech to text transcription',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'audio',
    rateLimits: {
      rpm: 500,
    },
    size: '1.5B',
  },
  {
    id: 'tts-1',
    name: 'TTS-1',
    provider: 'openai',
    category: 'audio',
    description: 'Text to speech conversion',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'audio',
    rateLimits: {
      rpm: 500,
    },
    size: '1B',
  },
  // Anthropic Models (Colored Orange)
  {
    id: 'claude-3-5-sonnet-20241022',
    name: 'Claude 3.5 Sonnet (latest)',
    provider: 'anthropic',
    category: 'chat',
    description: 'Most intelligent model with high capability and fast response times',
    isConfigured: true,
    isReasoner: false,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 8192,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-5-haiku-20241022',
    name: 'Claude 3.5 Haiku (latest)',
    provider: 'anthropic',
    category: 'chat',
    description: 'Fastest model with high intelligence and quick response times',
    isConfigured: true,
    isReasoner: false,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 8192,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-opus-20240229',
    name: 'Claude 3 Opus (latest)',
    provider: 'anthropic',
    category: 'chat',
    description: 'Most powerful model for highly complex tasks with top-level intelligence and understanding',
    isConfigured: true,
    isReasoner: false,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 4096,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-sonnet-20240229',
    name: 'Claude 3 Sonnet',
    provider: 'anthropic',
    category: 'chat',
    description: 'Balanced model optimized for production deployments with strong utility',
    isConfigured: true,
    isReasoner: false,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 4096,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-haiku-20240307',
    name: 'Claude 3 Haiku',
    provider: 'anthropic',
    category: 'chat',
    description: 'Fastest and most compact model for near-instant responsiveness',
    isConfigured: true,
    isReasoner: false,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 4096,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-7-sonnet-20250219',
    name: 'Claude 3.7 Sonnet',
    provider: 'anthropic',
    category: 'chat',
    description: 'Most advanced Claude model with hybrid reasoning and extended thinking capabilities',
    isConfigured: true,
    isReasoner: true,
    color: '#F97316',
    endpoint: 'chat',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 8192,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-7-sonnet-20250219',
    name: 'Claude 3.7 Sonnet (Agent)',
    provider: 'anthropic',
    category: 'agent',
    description: 'Advanced Claude research agent with extended thinking and step-by-step reasoning',
    isConfigured: true,
    isReasoner: true,
    color: '#F97316',
    endpoint: 'agent',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 8192,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  // Llama Models (Colored Blue)
  {
    id: 'marco-o1',
    name: 'Marco O1',
    provider: 'llama',
    category: 'chat',
    description: "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '7.62B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwq',
    name: 'QwQ',
    provider: 'llama',
    category: 'chat',
    description: "QwQ is an experimental research model focused on advancing AI reasoning capabilities.",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '32.8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'llama3.1:8b',
    name: 'Llama 3.1',
    provider: 'llama',
    category: 'chat',
    description: "Llama 3.1 - Meta's latest 8b parameter model",
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'llama3.2',
    name: 'Llama 3.2',
    provider: 'llama',
    category: 'chat',
    description: "Llama 3.2 - Meta's latest 3b parameter model",
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '3B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'deepseek-coder-v2',
    name: 'DeepSeek Coder V2',
    provider: 'llama',
    category: 'chat',
    description: 'An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '33B',
  },
  {
    id: 'nemotron-mini',
    name: 'Nemotron Mini',
    provider: 'llama',
    category: 'chat',
    description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'nemotron',
    name: 'Nemotron',
    provider: 'llama',
    category: 'chat',
    description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'Mistral-nemo',
    name: 'Mistral Nemo',
    provider: 'llama',
    category: 'chat',
    description: 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'orca2',
    name: 'Orca 2',
    provider: 'llama',
    category: 'chat',
    description: "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning.",
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'samantha-mistral',
    name: 'Samantha Mistral',
    provider: 'llama',
    category: 'chat',
    description: 'A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'granite3.1-dense:latest',
    name: 'Granite 3.1 Dense',
    provider: 'llama',
    category: 'chat',
    description: 'The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '8B',
  },
  {
    id: 'granite3-dense:8b',
    name: 'Granite 3 Dense',
    provider: 'llama',
    category: 'chat',
    description: 'The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '8B',
  },
  {
    id: 'granite3-moe',
    name: 'Granite 3 MOE',
    provider: 'llama',
    category: 'chat',
    description: 'The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:3b',
    name: 'Qwen 2.5 Coder 3b',
    provider: 'llama',
    category: 'chat',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '3B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:14b',
    name: 'Qwen 2.5 Coder 14b',
    provider: 'llama',
    category: 'chat',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '14B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:32b',
    name: 'Qwen 2.5 Coder 32b',
    provider: 'llama',
    category: 'chat',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '32B',
    rateLimits: {
      tpm: 40000,
      rpm: 25,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'gemma3:12b',
    name: 'Gemma 3 12b',
    provider: 'llama',
    category: 'chat',
    description: 'Gemma 3 12b is a 12B parameter model that is optimized for function calling tasks.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'phi3.5',
    name: 'Phi 3.5',
    provider: 'llama',
    category: 'chat',
    description: 'A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '3.8B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'phi3:14b',
    name: 'Phi 3 14b',
    provider: 'llama',
    category: 'chat',
    description: 'Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '14B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'vanilj/Phi-4',
    name: 'Phi 4 14b',
    provider: 'llama',
    category: 'chat',
    description: 'Phi-4 is a family of state-of-the-art open models by Microsoft.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '14B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },

  // Gemini Models (Colored Purple)
  {
    id: 'gemini-1.5-pro',
    name: 'Gemini 1.5 Pro',
    provider: 'gemini',
    category: 'chat',
    description: 'Google Gemini 1.5 Pro model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'chat',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '1.5T',
  },
  {
    id: 'gemini-2.0-flash-exp',
    name: 'Gemini 2.0 Flash Exp',
    provider: 'gemini',
    category: 'chat',
    description: 'Google Gemini 2.0 Flash Exp model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'chat',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '2T',
  },
  {
    id: 'gemini-1.5-flash',
    name: 'Gemini 1.5 Flash',
    provider: 'gemini',
    category: 'chat',
    description: 'Google Gemini 1.5 Flash model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'chat',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '1.5T',
  },
  {
    id: 'gemini-1.5-flash-8b',
    name: 'Gemini 1.5 Flash-8B',
    provider: 'gemini',
    category: 'chat',
    description: 'Google Gemini 1.5 Flash-8B model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'chat',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '8B',
  },
  {
    id: 'gemini-1.0-pro',
    name: 'Gemini 1.0 Pro',
    provider: 'gemini',
    category: 'chat',
    description: 'Google Gemini 1.0 Pro model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 45,
    },
  },
  {
    id: 'codegemma',
    name: 'Code Gemma',
    provider: 'llama',
    category: 'chat',
    description: 'Local Code Gemma model (7b parameters) from Google via Ollama',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '7B',
  },
  {
    id: 'gemma2:9b',
    name: 'Gemma 2',
    provider: 'llama',
    category: 'chat',
    description: "Gemma 2 - Google's latest 9b parameter model",
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '9B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'gemma2:2b',
    name: 'Gemma 2 (2b)',
    provider: 'llama',
    category: 'chat',
    description: 'Local Gemma 2 model (2b parameters) from Google via Ollama',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '2B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'opencoder:8b',
    name: 'OpenCoder 8b',
    provider: 'llama',
    category: 'chat',
    description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'opencoder:1.5b',
    name: 'OpenCoder 1.5b',
    provider: 'llama',
    category: 'chat',
    description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '1.5B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'solar-pro',
    name: 'Solar Pro',
    provider: 'llama',
    category: 'chat',
    description: 'Solar Pro Preview: an advanced language model (LLM) with 22 billion parameters designed to fit into a single GPU.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '22B',
    rateLimits: {
      tpm: 40000,
      rpm: 25,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'athene-v2',
    name: 'Athene V2',
    provider: 'llama',
    category: 'chat',
    description: 'Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '72B',
    rateLimits: {
      tpm: 30000,
      rpm: 20,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'yi-coder',
    name: 'Yi Coder',
    provider: 'llama',
    category: 'chat',
    description: 'Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '10B',
  },
  {
    id: 'mxbai-embed-large',
    name: 'MXBai Embed Large',
    provider: 'llama',
    category: 'embedding',
    description: 'State-of-the-art large embedding model from mixedbread.ai',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'chat',
    size: '1.5B',
  },
  // Grok Models (Colored Blue)
  {
    id: 'grok-beta',
    name: 'Grok Beta',
    provider: 'grok',
    category: 'chat',
    description: 'Comparable performance to Grok 2 but with improved efficiency, speed and capabilities',
    isConfigured: true,
    isReasoner: false,
    color: '#1DA1F2', // X/Twitter blue color
    endpoint: 'chat',
    rateLimits: {
      tpm: 100000,
      rpm: 500,
      rpd: 10000,
      tpd: 1000000,
    },
    size: '314B',
  },
  {
    id: 'gpt-4-turbo',
    name: 'GPT-4 Turbo (Agent)',
    provider: 'openai',
    category: 'agent',
    description: 'Enhanced GPT-4 with higher rate limits',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'agent',
    rateLimits: {
      tpm: 30000,
      rpm: 500,
      tpd: 90000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-4o',
    name: 'GPT-4 Optimized (Agent)',
    provider: 'openai',
    category: 'agent',
    description: 'Optimized version of GPT-4',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'agent',
    rateLimits: {
      tpm: 30000,
      rpm: 500,
      tpd: 90000,
    },
    size: '1.76T',
  },
  {
    id: 'gpt-4o-mini',
    name: 'GPT-4 Optimized Mini (Agent)',
    provider: 'openai',
    category: 'agent',
    description: 'Lightweight GPT-4 with higher token limits',
    isConfigured: true,
    isReasoner: false,
    color: '#3B7443',
    endpoint: 'agent',
    rateLimits: {
      tpm: 200000,
      rpm: 500,
      rpd: 10000,
      tpd: 2000000,
    },
    size: '1.76T',
  },
  {
    id: 'claude-3-5-sonnet-20241022',
    name: 'Claude 3 Sonnet (Agent)',
    provider: 'anthropic',
    category: 'agent',
    description: 'Balanced Claude model for agent tasks with strong performance',
    isConfigured: true,
    isReasoner: true,
    color: '#F97316',
    endpoint: 'agent',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 4096,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'claude-3-5-haiku-20241022',
    name: 'Claude 3 Haiku (Agent)',
    provider: 'anthropic',
    category: 'agent',
    description: 'Fast Claude model for quick agent responses',
    isConfigured: true,
    isReasoner: true,
    color: '#F97316',
    endpoint: 'agent',
    rateLimits: {
      maxInputTokens: 200000,
      maxOutputTokens: 4096,
      tpm: 100000,
      rpm: 50,
    },
    size: '175B',
  },
  {
    id: 'llama3.1:8b-agent',
    name: 'Llama 3.1 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "Llama 3.1 - Meta's latest 8b parameter model optimized for agent tasks",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'marco-o1-agent',
    name: 'Marco O1 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '7.62B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwq-agent',
    name: 'QwQ (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "QwQ is an experimental research model focused on advancing AI reasoning capabilities.",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '32.8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'llama3.2-agent',
    name: 'Llama 3.2 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "Llama 3.2 - Meta's latest 3b parameter model",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '3B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'deepseek-coder-v2-agent',
    name: 'DeepSeek Coder V2 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '33B',
  },
  {
    id: 'nemotron-mini-agent',
    name: 'Nemotron Mini (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'nemotron-agent',
    name: 'Nemotron (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'Mistral-nemo-agent',
    name: 'Mistral Nemo (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'orca2-agent',
    name: 'Orca 2 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning.",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'granite3.1-dense:latest',
    name: 'Granite 3.1 Dense',
    provider: 'llama',
    category: 'agent',
    description: 'The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBM’s initial testing.',
    isConfigured: true,
    isReasoner: false,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '8B',
  },
  {
    id: 'granite3-dense:8b-agent',
    name: 'Granite 3 Dense (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '8B',
  },
  {
    id: 'granite3-moe-agent',
    name: 'Granite 3 MOE (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:3b-agent',
    name: 'Qwen 2.5 Coder 3b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '3B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:14b-agent',
    name: 'Qwen 2.5 Coder 14b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '14B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'qwen2.5-coder:32b-agent',
    name: 'Qwen 2.5 Coder 32b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '32B',
    rateLimits: {
      tpm: 40000,
      rpm: 25,
      maxInputTokens: 128000,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'phi3.5-agent',
    name: 'Phi 3.5 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '3.8B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'phi3:14b-agent',
    name: 'Phi 3 14b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '14B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'codegemma-agent',
    name: 'Code Gemma (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'Local Code Gemma model (7b parameters) from Google via Ollama',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
    size: '7B',
  },
  {
    id: 'gemma2:9b-agent',
    name: 'Gemma 2 (Agent)',
    provider: 'llama',
    category: 'agent',
    description: "Gemma 2 - Google's latest 9b parameter model",
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '9B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'gemma2:2b-agent',
    name: 'Gemma 2 2b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'Local Gemma 2 model (2b parameters) from Google via Ollama',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '2B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'opencoder:8b-agent',
    name: 'OpenCoder 8b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '8B',
    rateLimits: {
      tpm: 60000,
      rpm: 35,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  {
    id: 'opencoder:1.5b-agent',
    name: 'OpenCoder 1.5b (Agent)',
    provider: 'llama',
    category: 'agent',
    description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
    isConfigured: true,
    isReasoner: true,
    color: '#8B5CF6',
    endpoint: 'agent',
    size: '1.5B',
    rateLimits: {
      tpm: 80000,
      rpm: 45,
      maxInputTokens: 16384,
      maxOutputTokens: 4096,
    },
  },
  // Grok Models (Colored Blue)
  {
    id: 'grok-beta-agent',
    name: 'Grok Beta (Agent)',
    provider: 'grok',
    category: 'agent',
    description: 'Comparable performance to Grok 2 but with improved efficiency, speed and capabilities',
    isConfigured: true,
    isReasoner: false,
    color: '#1DA1F2', // X/Twitter blue color
    endpoint: 'agent',
    rateLimits: {
      tpm: 100000,
      rpm: 500,
      rpd: 10000,
      tpd: 1000000,
    },
    size: '314B',
  },
  {
    id: 'gemini-2.0-flash-exp',
    name: 'Gemini 2.0 Flash Exp',
    provider: 'gemini',
    category: 'agent',
    description: 'Google Gemini 2.0 Flash Exp model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'agent',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '2T',
  },
  {
    id: 'gemini-1.5-pro-agent',
    name: 'Gemini 1.5 Pro (Agent)',
    provider: 'gemini',
    category: 'agent',
    description: 'Google Gemini 1.5 Pro model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'agent',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '1.5T',
  },
  {
    id: 'gemini-1.5-flash-agent',
    name: 'Gemini 1.5 Flash (Agent)',
    provider: 'gemini',
    category: 'agent',
    description: 'Google Gemini 1.5 Flash model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'agent',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '1.5T',
  },
  {
    id: 'gemini-1.5-flash-8b-agent',
    name: 'Gemini 1.5 Flash-8B (Agent)',
    provider: 'gemini',
    category: 'agent',
    description: 'Google Gemini 1.5 Flash-8B model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'agent',
    rateLimits: {
      tpm: 100000,
      rpm: 60,
    },
    size: '8B',
  },
  {
    id: 'gemini-1.0-pro-agent',
    name: 'Gemini 1.0 Pro (Agent)',
    provider: 'gemini',
    category: 'agent',
    description: 'Google Gemini 1.0 Pro model optimized for agent tasks.',
    isConfigured: true,
    isReasoner: true,
    color: '#4285F4',
    endpoint: 'agent',
    rateLimits: {
      tpm: 60000,
      rpm: 45,
    },
  }
];
