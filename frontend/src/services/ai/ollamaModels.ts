import { AIModel } from '../../types/ai';

export const OLLAMA_MODELS: AIModel[] = [
    {
        id: 'marco-o1',
        name: 'Marco O1',
        provider: 'ollama',
        category: 'chat',
        description: "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '7.62B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwq',
        name: 'QwQ',
        provider: 'ollama',
        category: 'chat',
        description: "QwQ is an experimental research model focused on advancing AI reasoning capabilities.",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '32.8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'llama3.1:8b',
        name: 'Llama 3.1',
        provider: 'ollama',
        category: 'chat',
        description: "Llama 3.1 - Meta's latest 8b parameter model",
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'llama3.2',
        name: 'Llama 3.2',
        provider: 'ollama',
        category: 'chat',
        description: "Llama 3.2 - Meta's latest 3b parameter model",
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '3B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'deepseek-coder-v2',
        name: 'DeepSeek Coder V2',
        provider: 'ollama',
        category: 'chat',
        description: 'An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '33B',
    },
    {
        id: 'nemotron-mini',
        name: 'Nemotron Mini',
        provider: 'ollama',
        category: 'chat',
        description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'nemotron',
        name: 'Nemotron',
        provider: 'ollama',
        category: 'chat',
        description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'Mistral-nemo',
        name: 'Mistral Nemo',
        provider: 'ollama',
        category: 'chat',
        description: 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'orca2',
        name: 'Orca 2',
        provider: 'ollama',
        category: 'chat',
        description: "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning.",
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'samantha-mistral',
        name: 'Samantha Mistral',
        provider: 'ollama',
        category: 'chat',
        description: 'A companion assistant trained in philosophy, psychology, and personal relationships. Based on Mistral.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'granite3.3',
        name: 'Granite 3.3',
        provider: 'ollama',
        category: 'chat',
        description: 'The latest series of Granite models, with significant improvements in performance and speed.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '8.17B',
    },
    {
        id: 'granite3.2-vision',
        name: 'Granite 3.2 Vision',
        provider: 'ollama',
        category: 'image',
        description: 'The latest series of Granite models, with significant improvements in performance and speed.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'images',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
        },
    },
    {
        id: 'granite3.1-dense:latest',
        name: 'Granite 3.1 Dense',
        provider: 'ollama',
        category: 'chat',
        description: 'The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBMs initial testing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '8B',
    },
    {
        id: 'granite3-dense:8b',
        name: 'Granite 3 Dense',
        provider: 'ollama',
        category: 'chat',
        description: 'The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '8B',
    },
    {
        id: 'granite3-moe',
        name: 'Granite 3 MOE',
        provider: 'ollama',
        category: 'chat',
        description: 'The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen3',
        name: 'Qwen 3',
        provider: 'ollama',
        category: 'chat',
        description: 'The latest series of Qwen models, with significant improvements in performance and speed.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '8B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:3b',
        name: 'Qwen 2.5 Coder 3b',
        provider: 'ollama',
        category: 'chat',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '3B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:14b',
        name: 'Qwen 2.5 Coder 14b',
        provider: 'ollama',
        category: 'chat',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '14B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:32b',
        name: 'Qwen 2.5 Coder 32b',
        provider: 'ollama',
        category: 'chat',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '32B',
        rateLimits: {
            tpm: 40000,
            rpm: 25,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'gemma3:12b',
        name: 'Gemma 3 12b',
        provider: 'ollama',
        category: 'chat',
        description: 'Gemma 3 12b is a 12B parameter model that is optimized for function calling tasks.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'phi3.5',
        name: 'Phi 3.5',
        provider: 'ollama',
        category: 'chat',
        description: 'A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '3.8B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'phi3:14b',
        name: 'Phi 3 14b',
        provider: 'ollama',
        category: 'chat',
        description: 'Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '14B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'vanilj/Phi-4',
        name: 'Phi 4 14b',
        provider: 'ollama',
        category: 'chat',
        description: 'Phi-4 is a family of state-of-the-art open models by Microsoft.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '14B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'codegemma',
        name: 'Code Gemma',
        provider: 'ollama',
        category: 'chat',
        description: 'Local Code Gemma model (7b parameters) from Google via Ollama',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '7B',
    },
    {
        id: 'gemma2:9b',
        name: 'Gemma 2',
        provider: 'ollama',
        category: 'chat',
        description: "Gemma 2 - Google's latest 9b parameter model",
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '9B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'gemma2:2b',
        name: 'Gemma 2 (2b)',
        provider: 'ollama',
        category: 'chat',
        description: 'Local Gemma 2 model (2b parameters) from Google via Ollama',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '2B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'opencoder:8b',
        name: 'OpenCoder 8b',
        provider: 'ollama',
        category: 'chat',
        description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'opencoder:1.5b',
        name: 'OpenCoder 1.5b',
        provider: 'ollama',
        category: 'chat',
        description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '1.5B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'solar-pro',
        name: 'Solar Pro',
        provider: 'ollama',
        category: 'chat',
        description: 'Solar Pro Preview: an advanced language model (LLM) with 22 billion parameters designed to fit into a single GPU.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '22B',
        rateLimits: {
            tpm: 40000,
            rpm: 25,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'athene-v2',
        name: 'Athene V2',
        provider: 'ollama',
        category: 'chat',
        description: 'Athene-V2 is a 72B parameter model which excels at code completion, mathematics, and log extraction tasks.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '72B',
        rateLimits: {
            tpm: 30000,
            rpm: 20,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'yi-coder',
        name: 'Yi Coder',
        provider: 'ollama',
        category: 'chat',
        description: 'Yi-Coder is a series of open-source code language models that delivers state-of-the-art coding performance with fewer than 10 billion parameters.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '10B',
    },
    {
        id: 'mxbai-embed-large',
        name: 'MXBai Embed Large',
        provider: 'ollama',
        category: 'embedding',
        description: 'State-of-the-art large embedding model from mixedbread.ai',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'chat',
        size: '1.5B',
    },
    {
        id: 'llama3.1:8b-agent',
        name: 'Llama 3.1 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "Llama 3.1 - Meta's latest 8b parameter model optimized for agent tasks",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'marco-o1-agent',
        name: 'Marco O1 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "An open large reasoning model for real-world solutions by the Alibaba International Digital Commerce Group (AIDC-AI).",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '7.62B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwq-agent',
        name: 'QwQ (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "QwQ is an experimental research model focused on advancing AI reasoning capabilities.",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '32.8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'llama3.2-agent',
        name: 'Llama 3.2 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "Llama 3.2 - Meta's latest 3b parameter model",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '3B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'deepseek-coder-v2-agent',
        name: 'DeepSeek Coder V2 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'An open-source Mixture-of-Experts code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '33B',
    },
    {
        id: 'nemotron-mini-agent',
        name: 'Nemotron Mini (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'nemotron-agent',
        name: 'Nemotron (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'A commercial-friendly small language model by NVIDIA optimized for roleplay, RAG QA, and function calling.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'Mistral-nemo-agent',
        name: 'Mistral Nemo (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'orca2-agent',
        name: 'Orca 2 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "Orca 2 is built by Microsoft research, and are a fine-tuned version of Meta's Llama 2 models. The model is designed to excel particularly in reasoning.",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'granite3.1-dense:latest',
        name: 'Granite 3.1 Dense',
        provider: 'ollama',
        category: 'agent',
        description: 'The IBM Granite 2B and 8B models are text-only dense LLMs trained on over 12 trillion tokens of data, demonstrated significant improvements over their predecessors in performance and speed in IBMs initial testing.',
        isConfigured: true,
        isReasoner: false,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '8B',
    },
    {
        id: 'granite3-dense:8b-agent',
        name: 'Granite 3 Dense (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'The IBM Granite 2B and 8B models are designed to support tool-based use cases and support for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '8B',
    },
    {
        id: 'granite3-moe-agent',
        name: 'Granite 3 MOE (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'The IBM Granite 1B and 3B models are the first mixture of experts (MoE) Granite models from IBM designed for low latency usage.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:3b-agent',
        name: 'Qwen 2.5 Coder 3b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '3B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:14b-agent',
        name: 'Qwen 2.5 Coder 14b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '14B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'qwen2.5-coder:32b-agent',
        name: 'Qwen 2.5 Coder 32b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'The latest series of Code-Specific Qwen models, with significant improvements in code generation, code reasoning, and code fixing.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '32B',
        rateLimits: {
            tpm: 40000,
            rpm: 25,
            maxInputTokens: 128000,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'phi3.5-agent',
        name: 'Phi 3.5 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'A lightweight AI model with 3.8 billion parameters with performance overtaking similarly and larger sized models.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '3.8B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'phi3:14b-agent',
        name: 'Phi 3 14b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '14B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'codegemma-agent',
        name: 'Code Gemma (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'Local Code Gemma model (7b parameters) from Google via Ollama',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
        size: '7B',
    },
    {
        id: 'gemma2:9b-agent',
        name: 'Gemma 2 (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: "Gemma 2 - Google's latest 9b parameter model",
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '9B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'gemma2:2b-agent',
        name: 'Gemma 2 2b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'Local Gemma 2 model (2b parameters) from Google via Ollama',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '2B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'opencoder:8b-agent',
        name: 'OpenCoder 8b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '8B',
        rateLimits: {
            tpm: 60000,
            rpm: 35,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
    {
        id: 'opencoder:1.5b-agent',
        name: 'OpenCoder 1.5b (Agent)',
        provider: 'ollama',
        category: 'agent',
        description: 'OpenCoder is an open and reproducible code LLM family which includes 1.5B and 8B models, supporting both chat and fill-in-the-middle for English and Chinese languages.',
        isConfigured: true,
        isReasoner: true,
        color: '#8B5CF6',
        endpoint: 'agent',
        size: '1.5B',
        rateLimits: {
            tpm: 80000,
            rpm: 45,
            maxInputTokens: 16384,
            maxOutputTokens: 4096,
        },
    },
]; 