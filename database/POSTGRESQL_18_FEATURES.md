# PostgreSQL 18 Features Implementation Guide for Second Brain

This comprehensive guide documents PostgreSQL 18's new features and provides step-by-step implementation instructions tailored for the Second Brain application.

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [High-Impact Features for Second Brain](#2-high-impact-features-for-second-brain)
3. [JSON/JSONB Enhancements](#3-jsonjsonb-enhancements)
4. [SQL Statement Enhancements](#4-sql-statement-enhancements)
5. [Temporal Features](#5-temporal-features)
6. [Full-Text Search Improvements](#6-full-text-search-improvements)
7. [Performance Tuning](#7-performance-tuning)
8. [Monitoring & Observability](#8-monitoring--observability)
9. [Implementation Roadmap](#9-implementation-roadmap)
10. [Migration Scripts](#10-migration-scripts)
11. [Backend Code Updates](#11-backend-code-updates)
12. [Testing & Validation](#12-testing--validation)

---

## 1. Executive Summary

### PostgreSQL 18 Release Highlights (September 2025)

PostgreSQL 18 represents a major leap forward with features specifically beneficial for AI/ML workloads, knowledge management systems, and RAG (Retrieval-Augmented Generation) applications like Second Brain.

### Key Benefits for Second Brain

| Feature | Impact Area | Expected Improvement |
|---------|-------------|---------------------|
| **UUIDv7** | Primary Keys | 4x faster index performance |
| **Async I/O (AIO)** | Vector Search | 2-3x faster sequential scans |
| **pgvector 0.8** | RAG Pipeline | 25-40% better filtered queries |
| **JSON_TABLE** | Agent Tools | Cleaner JSON querying |
| **MERGE with RETURNING** | Embeddings Upsert | Single atomic operation |
| **Virtual Generated Columns** | Full-Text Search | Reduced trigger overhead |
| **Parallel GIN Builds** | Index Creation | 50-70% faster index builds |
| **Skip Scan** | Multi-column Indexes | Better composite index usage |

### Current Stack Compatibility

| Component | Current Version | Required for PG18 |
|-----------|-----------------|-------------------|
| PostgreSQL | 18.x | âœ… Already using |
| pgvector | 0.7.x | Upgrade to 0.8.x |
| Npgsql | 8.x | Upgrade to 10.x |
| EF Core | 8.x | Upgrade to 10.x |
| .NET | 8/9 | .NET 9 recommended |

---

## 2. High-Impact Features for Second Brain

### 2.1 UUIDv7 Migration

#### Current State

Second Brain currently uses `TEXT` primary keys for most tables:

```sql
-- Current schema (notes table)
CREATE TABLE notes (
    id TEXT PRIMARY KEY,  -- Random UUID as text
    ...
);
```

#### Problem with UUIDv4/TEXT

- Random distribution causes B-tree index fragmentation
- Poor cache locality during inserts
- Index page splits on every insert
- No inherent time ordering

#### UUIDv7 Benefits

UUIDv7 (RFC 9562) embeds a Unix timestamp in the first 48 bits:

```text
 0                   1                   2                   3
 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                           unix_ts_ms                          |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|           unix_ts_ms          |  ver  |       rand_a          |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|var|                        rand_b                              |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
|                            rand_b                              |
+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

#### Performance Advantages

- Sequential inserts append to end of B-tree
- 4x faster insert performance
- Better index scan locality
- Natural time-ordering without extra timestamp column
- Sortable by creation time

#### Implementation Steps

##### Step 1: New tables use UUIDv7 by default

```sql
-- New table definition pattern
CREATE TABLE new_table (
    id UUID PRIMARY KEY DEFAULT uuidv7(),
    ...
);
```

##### Step 2: Add UUIDv7 columns to existing tables (non-breaking)

```sql
-- Add new UUID column alongside existing TEXT id
ALTER TABLE notes ADD COLUMN uuid_id UUID DEFAULT uuidv7();

-- Backfill existing records (generates new UUIDv7 for each)
UPDATE notes SET uuid_id = uuidv7() WHERE uuid_id IS NULL;

-- Create index on new column
CREATE INDEX CONCURRENTLY ix_notes_uuid_id ON notes(uuid_id);
```

##### Step 3: EF Core Configuration

```csharp
// In ApplicationDbContext.cs or Program.cs
services.AddDbContext<ApplicationDbContext>(options =>
    options.UseNpgsql(connectionString, npgsqlOptions =>
    {
        npgsqlOptions.SetPostgresVersion(18, 0);
        // Enable UUIDv7 generation
    }));

// In entity configuration
modelBuilder.Entity<Note>(entity =>
{
    entity.Property(e => e.Id)
        .HasDefaultValueSql("uuidv7()");
});
```

##### Step 4: C# Code Updates

```csharp
// Using .NET 9's Guid.CreateVersion7()
public class Note
{
    public Guid Id { get; set; } = Guid.CreateVersion7();
    // ...
}

// Or let the database generate it
public class Note
{
    public Guid Id { get; set; } // Database default handles this
    // ...
}
```

#### Migration Strategy for Existing Data

For tables with existing data, use a phased approach:

```sql
-- Phase 1: Add new column
ALTER TABLE notes ADD COLUMN new_id UUID;

-- Phase 2: Generate UUIDv7 preserving time order from created_at
-- This maintains temporal ordering based on original creation time
UPDATE notes 
SET new_id = (
    -- Extract timestamp and convert to UUIDv7 format
    -- For simplicity, generate fresh UUIDv7 ordered by created_at
    uuidv7()
)
WHERE new_id IS NULL;

-- Phase 3: Add NOT NULL constraint
ALTER TABLE notes ALTER COLUMN new_id SET NOT NULL;

-- Phase 4: Create unique index
CREATE UNIQUE INDEX CONCURRENTLY ix_notes_new_id ON notes(new_id);

-- Phase 5: Application migration (update code to use new_id)
-- Phase 6: Drop old column and rename (during maintenance window)
-- ALTER TABLE notes DROP COLUMN id;
-- ALTER TABLE notes RENAME COLUMN new_id TO id;
-- ALTER TABLE notes ADD PRIMARY KEY (id);
```

---

### 2.2 Async I/O (AIO) Subsystem

#### Overview

PostgreSQL 18's AIO subsystem allows multiple I/O requests to be issued concurrently, dramatically improving performance for:

- Sequential scans (vector similarity searches)
- Bitmap heap scans (hybrid search)
- VACUUM operations
- Bulk data loading

#### Configuration Parameters

| Parameter | Description | Recommended Value |
|-----------|-------------|-------------------|
| `io_method` | I/O handling method | `worker` (cross-platform) or `io_uring` (Linux 5.1+) |
| `io_workers` | Background I/O workers | `ceil(cpu_cores * 0.25)` |
| `effective_io_concurrency` | Concurrent I/O requests | `200` (SSD) / `2` (HDD) |
| `io_combine_limit` | I/O request batching | `128kB` default |
| `maintenance_io_concurrency` | VACUUM/CREATE INDEX | `10-20` |

#### Docker Configuration

```yaml
# docker-compose.yml
services:
  postgres:
    image: pgvector/pgvector:pg18
    command:
      - "postgres"
      - "-c" 
      - "io_method=worker"
      - "-c"
      - "io_workers=4"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "maintenance_io_concurrency=10"
```

#### PostgreSQL Configuration File

```ini
# postgresql.conf additions for PostgreSQL 18

# Async I/O Settings
io_method = 'worker'                    # Options: sync, worker, io_uring
io_workers = 4                          # ~25% of CPU cores
effective_io_concurrency = 200          # For SSDs
maintenance_io_concurrency = 10         # For VACUUM/INDEX builds

# If using Linux with kernel 5.1+, io_uring provides best performance:
# io_method = 'io_uring'
```

#### Monitoring AIO Performance

```sql
-- View active async I/O operations
SELECT * FROM pg_aios;

-- Check I/O statistics
SELECT * FROM pg_stat_io;

-- Monitor background workers
SELECT * FROM pg_stat_activity 
WHERE backend_type = 'io worker';
```

#### Expected Impact on Second Brain

| Operation | Before AIO | After AIO | Improvement |
|-----------|------------|-----------|-------------|
| Vector search (10K embeddings) | ~40ms | ~15ms | 2.7x |
| Full table scan | ~100ms | ~35ms | 2.9x |
| Bulk embedding insert | ~5s | ~2s | 2.5x |

---

### 2.3 pgvector 0.8 Improvements

#### Iterative Index Scans

**Problem (pgvector < 0.8):** When using filtered queries, pgvector could return fewer results than requested if many vectors were filtered out ("overfiltering").

```sql
-- This might return < 5 results if many notes are filtered
SELECT * FROM note_embeddings
WHERE user_id = 'user-123'
  AND embedding <=> query_vector < 0.5
ORDER BY embedding <=> query_vector
LIMIT 5;
```

**Solution (pgvector 0.8):** Iterative scanning continues until enough results are found.

#### HNSW Index Optimizations

```sql
-- Create optimized HNSW index for Second Brain embeddings
CREATE INDEX CONCURRENTLY ix_embeddings_hnsw 
ON note_embeddings 
USING hnsw (embedding vector_cosine_ops)
WITH (
    m = 16,              -- Connections per layer (default: 16)
    ef_construction = 64 -- Build-time search width (default: 64)
);

-- Set search-time parameter for quality/speed tradeoff
SET hnsw.ef_search = 100;  -- Higher = better recall, slower
```

#### Recommended pgvector Configuration

```sql
-- For RAG workloads with ~100K embeddings
-- Create index with tuned parameters
DROP INDEX IF EXISTS ix_embeddings_hnsw;

CREATE INDEX CONCURRENTLY ix_embeddings_hnsw
ON note_embeddings
USING hnsw (embedding vector_cosine_ops)
WITH (
    m = 24,               -- More connections for better recall
    ef_construction = 128 -- Higher quality index
);

-- Session-level search tuning
SET hnsw.ef_search = 100;

-- For IVFFlat alternative (lower memory, good for >1M vectors)
CREATE INDEX CONCURRENTLY ix_embeddings_ivfflat
ON note_embeddings
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- sqrt(num_vectors) is a good starting point
```

#### Upgrading pgvector

```bash
# macOS with Homebrew
brew upgrade pgvector

# Ubuntu/Debian
sudo apt update
sudo apt install postgresql-18-pgvector

# From source
git clone --branch v0.8.0 https://github.com/pgvector/pgvector.git
cd pgvector
make
sudo make install

# In PostgreSQL
ALTER EXTENSION vector UPDATE TO '0.8.0';
```

---

## 3. JSON/JSONB Enhancements

### 3.1 JSON_TABLE Function

#### JSON_TABLE Overview

`JSON_TABLE` transforms JSON data into relational rows, eliminating complex `jsonb_*` function chains.

#### Current Second Brain JSON Usage

```sql
-- Current: tool_calls stores JSON as TEXT
CREATE TABLE tool_calls (
    id TEXT PRIMARY KEY,
    message_id VARCHAR(128) NOT NULL,
    tool_name VARCHAR(100) NOT NULL,
    arguments TEXT NOT NULL,  -- JSON stored as TEXT
    result TEXT NOT NULL,     -- JSON stored as TEXT
    ...
);

-- Current: agent_capabilities stored as JSON text
CREATE TABLE chat_conversations (
    ...
    agent_capabilities TEXT,  -- JSON array: '["notes", "search"]'
    ...
);
```

#### JSON_TABLE Examples for Second Brain

##### Example 1: Parse Tool Call Arguments

```sql
-- Before (complex)
SELECT 
    tc.id,
    tc.tool_name,
    tc.arguments::jsonb ->> 'noteId' AS note_id,
    tc.arguments::jsonb ->> 'action' AS action
FROM tool_calls tc;

-- After (with JSON_TABLE) - cleaner and more performant
SELECT 
    tc.id,
    tc.tool_name,
    jt.*
FROM tool_calls tc,
JSON_TABLE(
    tc.arguments::jsonb,
    '$'
    COLUMNS (
        note_id TEXT PATH '$.noteId',
        action TEXT PATH '$.action',
        title TEXT PATH '$.title',
        content TEXT PATH '$.content'
    )
) AS jt;
```

##### Example 2: Expand Agent Capabilities Array

```sql
-- Get all conversations with their capabilities expanded
SELECT 
    cc.id,
    cc.title,
    cap.capability
FROM chat_conversations cc,
JSON_TABLE(
    cc.agent_capabilities::jsonb,
    '$[*]'
    COLUMNS (
        capability TEXT PATH '$'
    )
) AS cap
WHERE cc.agent_enabled = true;
```

##### Example 3: Parse Retrieved Notes Context

```sql
-- Analyze RAG context quality from retrieved notes
SELECT 
    cm.id AS message_id,
    rn.note_id,
    rn.relevance_score,
    JSON_TABLE(
        rn.chunk_content::jsonb,
        '$'
        COLUMNS (
            word_count INT PATH '$.wordCount',
            has_code BOOLEAN PATH '$.hasCode'
        )
    ) AS chunk_meta
FROM chat_messages cm
JOIN retrieved_notes rn ON cm.id = rn.message_id;
```

##### Example 4: Analytics Query - Tool Usage Statistics

```sql
-- Analyze tool call patterns
WITH tool_args AS (
    SELECT 
        tc.tool_name,
        tc.executed_at,
        tc.success,
        jt.*
    FROM tool_calls tc,
    JSON_TABLE(
        tc.arguments::jsonb,
        '$'
        COLUMNS (
            action TEXT PATH '$.action',
            query TEXT PATH '$.query',
            note_id TEXT PATH '$.noteId'
        )
    ) AS jt
)
SELECT 
    tool_name,
    action,
    COUNT(*) AS call_count,
    AVG(CASE WHEN success THEN 1 ELSE 0 END) AS success_rate
FROM tool_args
GROUP BY tool_name, action
ORDER BY call_count DESC;
```

### 3.2 Enhanced JSON Path Expressions

```sql
-- Find conversations where agent used specific tools
SELECT id, title
FROM chat_conversations
WHERE jsonb_path_exists(
    agent_capabilities::jsonb,
    '$[*] ? (@ == "notes")'
);

-- Find tool calls with errors in result
SELECT tc.id, tc.tool_name, tc.result
FROM tool_calls tc
WHERE jsonb_path_exists(
    tc.result::jsonb,
    '$ ? (@.error != null)'
);

-- Complex filtering on nested structures
SELECT *
FROM rag_query_logs
WHERE jsonb_path_exists(
    metadata::jsonb,
    '$.retrieval ? (@.score > 0.8 && @.source == "notes")'
);
```

### 3.3 Schema Migration to JSONB

Consider migrating TEXT JSON columns to native JSONB:

```sql
-- Step 1: Add JSONB columns
ALTER TABLE tool_calls 
    ADD COLUMN arguments_jsonb JSONB,
    ADD COLUMN result_jsonb JSONB;

-- Step 2: Migrate data
UPDATE tool_calls 
SET 
    arguments_jsonb = arguments::jsonb,
    result_jsonb = result::jsonb
WHERE arguments_jsonb IS NULL;

-- Step 3: Add GIN indexes for JSON queries
CREATE INDEX CONCURRENTLY ix_tool_calls_arguments_gin 
ON tool_calls USING GIN(arguments_jsonb jsonb_path_ops);

-- Step 4: Create validation constraint
ALTER TABLE tool_calls 
ADD CONSTRAINT chk_arguments_json 
CHECK (arguments_jsonb IS NOT NULL AND jsonb_typeof(arguments_jsonb) = 'object');
```

---

## 4. SQL Statement Enhancements

### 4.1 MERGE with RETURNING (OLD/NEW)

#### Current Upsert Pattern in PostgresVectorStore

```csharp
// Current: Two separate operations
var existingEmbedding = await _context.NoteEmbeddings
    .FirstOrDefaultAsync(e => e.Id == embedding.Id, cancellationToken);

if (existingEmbedding != null)
{
    // Update existing
    existingEmbedding.Content = embedding.Content;
    existingEmbedding.Embedding = embedding.Embedding;
    // ... more updates
}
else
{
    // Create new
    _context.NoteEmbeddings.Add(embedding);
}

await _context.SaveChangesAsync(cancellationToken);
```

#### New MERGE Pattern

```sql
-- Single atomic MERGE operation with change tracking
MERGE INTO note_embeddings AS target
USING (VALUES (
    @id, @note_id, @user_id, @chunk_index, @content, 
    @embedding, @provider, @model, @note_title, @note_tags, @note_updated_at
)) AS source (
    id, note_id, user_id, chunk_index, content, 
    embedding, provider, model, note_title, note_tags, note_updated_at
)
ON target.id = source.id
WHEN MATCHED THEN
    UPDATE SET
        note_id = source.note_id,
        user_id = source.user_id,
        chunk_index = source.chunk_index,
        content = source.content,
        embedding = source.embedding,
        embedding_provider = source.provider,
        embedding_model = source.model,
        note_title = source.note_title,
        note_tags = source.note_tags,
        note_updated_at = source.note_updated_at
WHEN NOT MATCHED THEN
    INSERT (id, note_id, user_id, chunk_index, content, embedding, 
            embedding_provider, embedding_model, note_title, note_tags, 
            note_updated_at, created_at)
    VALUES (source.id, source.note_id, source.user_id, source.chunk_index,
            source.content, source.embedding, source.provider, source.model,
            source.note_title, source.note_tags, source.note_updated_at, NOW())
RETURNING 
    merge_action() AS action,
    id,
    OLD.content AS old_content,
    NEW.content AS new_content,
    OLD.embedding AS old_embedding,
    NEW.embedding AS new_embedding;
```

#### Batch MERGE for Multiple Embeddings

```sql
-- Batch upsert with staging table
CREATE TEMP TABLE staging_embeddings (LIKE note_embeddings INCLUDING ALL);

-- Insert batch into staging
INSERT INTO staging_embeddings VALUES ...;

-- Single MERGE for entire batch
MERGE INTO note_embeddings AS target
USING staging_embeddings AS source
ON target.id = source.id
WHEN MATCHED AND target.note_updated_at < source.note_updated_at THEN
    UPDATE SET
        content = source.content,
        embedding = source.embedding,
        note_updated_at = source.note_updated_at
WHEN NOT MATCHED THEN
    INSERT (id, note_id, user_id, chunk_index, content, embedding,
            embedding_provider, embedding_model, note_title, note_tags,
            note_updated_at, created_at)
    VALUES (source.id, source.note_id, source.user_id, source.chunk_index,
            source.content, source.embedding, source.embedding_provider,
            source.embedding_model, source.note_title, source.note_tags,
            source.note_updated_at, NOW())
RETURNING merge_action(), id;

DROP TABLE staging_embeddings;
```

### 4.2 Virtual Generated Columns

#### Current: Trigger-Based Search Vector

```sql
-- Current implementation in 08_search_vectors.sql
CREATE OR REPLACE FUNCTION update_note_embedding_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector := 
        setweight(to_tsvector('english', COALESCE(NEW.note_title, '')), 'A') ||
        setweight(to_tsvector('english', COALESCE(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_note_embedding_search_vector
    BEFORE INSERT OR UPDATE OF note_title, content ON note_embeddings
    FOR EACH ROW
    EXECUTE FUNCTION update_note_embedding_search_vector();
```

#### New: Virtual Generated Column (PostgreSQL 18)

##### Option A: Virtual (computed at query time, no storage)

```sql
-- Note: Virtual generated columns CANNOT be indexed
-- Use for display purposes or infrequently accessed computed values
ALTER TABLE note_embeddings
ADD COLUMN search_display TEXT 
GENERATED ALWAYS AS (
    note_title || ': ' || LEFT(content, 100)
) VIRTUAL;
```

##### Option B: Stored Generated Column (for indexable search_vector)

```sql
-- For tsvector that needs GIN index, use STORED
-- Drop existing trigger-based implementation
DROP TRIGGER IF EXISTS trg_note_embedding_search_vector ON note_embeddings;
DROP FUNCTION IF EXISTS update_note_embedding_search_vector();

-- Add stored generated column
ALTER TABLE note_embeddings
DROP COLUMN IF EXISTS search_vector;

ALTER TABLE note_embeddings
ADD COLUMN search_vector tsvector 
GENERATED ALWAYS AS (
    setweight(to_tsvector('english', COALESCE(note_title, '')), 'A') ||
    setweight(to_tsvector('english', COALESCE(content, '')), 'B')
) STORED;

-- Recreate GIN index
CREATE INDEX CONCURRENTLY idx_note_embeddings_search_vector 
ON note_embeddings USING GIN(search_vector);
```

#### Benefits of Generated Columns

| Aspect | Trigger | Virtual Generated | Stored Generated |
|--------|---------|-------------------|------------------|
| Storage | Stored | None | Stored |
| Compute | On INSERT/UPDATE | On SELECT | On INSERT/UPDATE |
| Indexable | Yes | No | Yes |
| Maintenance | Manual | Automatic | Automatic |
| Performance | Good | Best for rare access | Best for frequent access |

### 4.3 Skip Scan Lookups

#### Skip Scan Overview

Skip scan allows PostgreSQL to efficiently use a composite index even when the leading column has few distinct values or is not in the WHERE clause.

#### Second Brain Index Optimization

```sql
-- Current composite index
CREATE INDEX ix_notes_user_updated ON notes(user_id, updated_at DESC);

-- Query that benefits from skip scan in PG18
-- Even without user_id filter, can use the index
SELECT * FROM notes
WHERE updated_at > NOW() - INTERVAL '7 days'
ORDER BY updated_at DESC
LIMIT 100;

-- Additional indexes optimized for skip scan
CREATE INDEX ix_embeddings_provider_user 
ON note_embeddings(embedding_provider, user_id, note_id);

-- This query can now use skip scan even without embedding_provider filter
SELECT DISTINCT user_id FROM note_embeddings;
```

#### Verifying Skip Scan Usage

```sql
-- Check if skip scan is being used
EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
SELECT DISTINCT user_id FROM note_embeddings;

-- Look for "Index Skip Scan" in the plan output
```

---

## 5. Temporal Features

### 5.1 Temporal Constraints (WITHOUT OVERLAPS)

#### Use Case: Note Version History

Track non-overlapping validity periods for note versions:

```sql
-- Create note versions table with temporal constraint
CREATE TABLE note_versions (
    note_id UUID NOT NULL,
    valid_period TSTZRANGE NOT NULL,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    tags TEXT[] NOT NULL DEFAULT '{}',
    modified_by VARCHAR(128) NOT NULL,
    
    -- Temporal primary key: no overlapping versions for same note
    PRIMARY KEY (note_id, valid_period WITHOUT OVERLAPS)
);

-- Insert a new version (automatically ensures no overlap)
INSERT INTO note_versions (note_id, valid_period, title, content, tags, modified_by)
VALUES (
    '01234567-89ab-cdef-0123-456789abcdef',
    tstzrange(NOW(), NULL, '[)'),  -- Valid from now, no end
    'My Note Title',
    'Note content...',
    ARRAY['tag1', 'tag2'],
    'user-123'
);

-- Close current version and create new one
UPDATE note_versions
SET valid_period = tstzrange(lower(valid_period), NOW(), '[)')
WHERE note_id = '01234567-89ab-cdef-0123-456789abcdef'
  AND upper_inf(valid_period);

INSERT INTO note_versions (note_id, valid_period, title, content, tags, modified_by)
VALUES (
    '01234567-89ab-cdef-0123-456789abcdef',
    tstzrange(NOW(), NULL, '[)'),
    'Updated Title',
    'Updated content...',
    ARRAY['tag1', 'tag2', 'tag3'],
    'user-123'
);
```

#### Use Case: Chat Session Tracking

```sql
-- Track active chat sessions with temporal constraints
CREATE TABLE chat_sessions (
    user_id VARCHAR(128) NOT NULL,
    conversation_id UUID NOT NULL,
    session_period TSTZRANGE NOT NULL,
    device_info JSONB,
    
    -- User can only have one active session per conversation at a time
    PRIMARY KEY (user_id, conversation_id, session_period WITHOUT OVERLAPS)
);

-- Query: Get note content at a specific point in time
SELECT title, content, tags
FROM note_versions
WHERE note_id = '01234567-89ab-cdef-0123-456789abcdef'
  AND valid_period @> '2025-06-15 10:30:00+00'::timestamptz;

-- Query: Get full history of a note
SELECT 
    lower(valid_period) AS version_start,
    upper(valid_period) AS version_end,
    title,
    LEFT(content, 100) AS content_preview,
    modified_by
FROM note_versions
WHERE note_id = '01234567-89ab-cdef-0123-456789abcdef'
ORDER BY lower(valid_period) DESC;
```

### 5.2 Temporal Foreign Keys

```sql
-- Address validity periods
CREATE TABLE user_addresses (
    user_id VARCHAR(128) NOT NULL,
    valid_period TSTZRANGE NOT NULL,
    address_line1 TEXT NOT NULL,
    city TEXT NOT NULL,
    
    PRIMARY KEY (user_id, valid_period WITHOUT OVERLAPS)
);

-- Orders reference valid address during order time
CREATE TABLE orders (
    order_id UUID PRIMARY KEY DEFAULT uuidv7(),
    user_id VARCHAR(128) NOT NULL,
    order_period TSTZRANGE NOT NULL,
    total_amount DECIMAL(10,2) NOT NULL,
    
    -- Temporal foreign key: order period must be within address validity
    FOREIGN KEY (user_id, PERIOD order_period)
        REFERENCES user_addresses (user_id, PERIOD valid_period)
);
```

---

## 6. Full-Text Search Improvements

### 6.1 Native PostgreSQL BM25

#### Current Implementation (In-Memory)

The current `BM25SearchService.cs` performs BM25 scoring in memory:

```csharp
// Current: Load all embeddings and score in memory
var embeddings = (await _repository.GetWithSearchVectorAsync()).ToList();
var searchResults = embeddings
    .Select(e => new { Embedding = e, Score = CalculateBM25Score(...) })
    .Where(x => x.Score > 0)
    .OrderByDescending(x => x.Score)
    .Take(topK);
```

#### New: Native PostgreSQL Full-Text Search

```sql
-- Use ts_rank_cd (cover density) for BM25-like ranking
SELECT 
    ne.id,
    ne.note_id,
    ne.content,
    ne.note_title,
    ne.note_tags,
    ne.chunk_index,
    ts_rank_cd(
        ne.search_vector,
        plainto_tsquery('english', @query),
        32  -- Normalization: divide by document length
    ) AS bm25_score
FROM note_embeddings ne
WHERE ne.user_id = @user_id
  AND ne.search_vector @@ plainto_tsquery('english', @query)
ORDER BY bm25_score DESC
LIMIT @top_k;
```

#### Advanced: Weighted BM25 with Headline

```sql
-- Full-text search with highlighted snippets
SELECT 
    ne.id,
    ne.note_id,
    ne.note_title,
    ts_rank_cd(ne.search_vector, query, 32) AS score,
    ts_headline(
        'english',
        ne.content,
        query,
        'StartSel=<mark>, StopSel=</mark>, MaxWords=50, MinWords=25, MaxFragments=3'
    ) AS highlighted_content
FROM note_embeddings ne,
     plainto_tsquery('english', @search_query) query
WHERE ne.user_id = @user_id
  AND ne.search_vector @@ query
ORDER BY score DESC
LIMIT 10;
```

#### Phrase Search Support

```sql
-- Exact phrase search
SELECT * FROM note_embeddings
WHERE search_vector @@ phraseto_tsquery('english', 'machine learning');

-- Proximity search (words within distance)
SELECT * FROM note_embeddings
WHERE search_vector @@ to_tsquery('english', 'neural <2> network');
```

### 6.2 Parallel GIN Index Builds

```sql
-- Configure parallel workers for index builds
SET max_parallel_maintenance_workers = 4;

-- Create GIN index in parallel
CREATE INDEX CONCURRENTLY idx_embeddings_search_gin
ON note_embeddings USING GIN(search_vector);

-- Monitor progress
SELECT 
    p.phase,
    p.blocks_total,
    p.blocks_done,
    p.tuples_total,
    p.tuples_done,
    round(100.0 * p.blocks_done / nullif(p.blocks_total, 0), 1) AS pct_complete
FROM pg_stat_progress_create_index p;
```

### 6.3 Hybrid Search with Native BM25

```sql
-- Complete hybrid search query using RRF
WITH vector_results AS (
    SELECT 
        id,
        note_id,
        content,
        note_title,
        note_tags,
        chunk_index,
        1 - (embedding <=> @query_embedding::vector) AS vector_score,
        ROW_NUMBER() OVER (ORDER BY embedding <=> @query_embedding::vector) AS vector_rank
    FROM note_embeddings
    WHERE user_id = @user_id
    ORDER BY embedding <=> @query_embedding::vector
    LIMIT 20
),
bm25_results AS (
    SELECT 
        id,
        note_id,
        content,
        note_title,
        note_tags,
        chunk_index,
        ts_rank_cd(search_vector, plainto_tsquery('english', @query), 32) AS bm25_score,
        ROW_NUMBER() OVER (
            ORDER BY ts_rank_cd(search_vector, plainto_tsquery('english', @query), 32) DESC
        ) AS bm25_rank
    FROM note_embeddings
    WHERE user_id = @user_id
      AND search_vector @@ plainto_tsquery('english', @query)
    LIMIT 20
)
SELECT 
    COALESCE(v.id, b.id) AS id,
    COALESCE(v.note_id, b.note_id) AS note_id,
    COALESCE(v.content, b.content) AS content,
    COALESCE(v.note_title, b.note_title) AS note_title,
    COALESCE(v.note_tags, b.note_tags) AS note_tags,
    COALESCE(v.chunk_index, b.chunk_index) AS chunk_index,
    COALESCE(v.vector_score, 0) AS vector_score,
    COALESCE(b.bm25_score, 0) AS bm25_score,
    COALESCE(v.vector_rank, 1000) AS vector_rank,
    COALESCE(b.bm25_rank, 1000) AS bm25_rank,
    -- RRF Score: 1/(k+rank) for each ranking
    (COALESCE(1.0 / (60 + v.vector_rank), 0) * @vector_weight +
     COALESCE(1.0 / (60 + b.bm25_rank), 0) * @bm25_weight) AS rrf_score,
    v.id IS NOT NULL AS found_in_vector,
    b.id IS NOT NULL AS found_in_bm25
FROM vector_results v
FULL OUTER JOIN bm25_results b ON v.id = b.id
ORDER BY rrf_score DESC
LIMIT @top_k;
```

---

## 7. Performance Tuning

### 7.1 PostgreSQL 18 Configuration

```ini
# postgresql.conf - Optimized for Second Brain workloads

#------------------------------------------------------------------------------
# CONNECTIONS AND AUTHENTICATION
#------------------------------------------------------------------------------
max_connections = 100
superuser_reserved_connections = 3

#------------------------------------------------------------------------------
# RESOURCE USAGE (except WAL)
#------------------------------------------------------------------------------
# Memory Settings (adjust based on available RAM)
shared_buffers = 4GB                    # 25% of RAM for dedicated DB server
effective_cache_size = 12GB             # 75% of RAM
maintenance_work_mem = 1GB              # For VACUUM, CREATE INDEX
work_mem = 64MB                         # Per-operation sort/hash memory

# Async I/O (PostgreSQL 18)
io_method = 'worker'                    # 'io_uring' on Linux 5.1+
io_workers = 4                          # ceil(cpu_cores * 0.25)
effective_io_concurrency = 200          # For SSDs

# Background Writer
bgwriter_delay = 200ms
bgwriter_lru_maxpages = 100
bgwriter_lru_multiplier = 2.0

#------------------------------------------------------------------------------
# PARALLEL QUERY
#------------------------------------------------------------------------------
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4    # For parallel index builds
parallel_leader_participation = on

#------------------------------------------------------------------------------
# QUERY TUNING
#------------------------------------------------------------------------------
random_page_cost = 1.1                  # For SSDs (default 4.0 for HDDs)
effective_io_concurrency = 200          # SSDs
default_statistics_target = 100

# JIT Compilation
jit = on
jit_above_cost = 100000
jit_inline_above_cost = 500000
jit_optimize_above_cost = 500000

#------------------------------------------------------------------------------
# WRITE-AHEAD LOG
#------------------------------------------------------------------------------
wal_level = replica
max_wal_size = 2GB
min_wal_size = 1GB
checkpoint_completion_target = 0.9

#------------------------------------------------------------------------------
# AUTOVACUUM
#------------------------------------------------------------------------------
autovacuum = on
autovacuum_max_workers = 4
autovacuum_naptime = 30s
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.1
autovacuum_analyze_scale_factor = 0.05

#------------------------------------------------------------------------------
# LOGGING
#------------------------------------------------------------------------------
log_destination = 'stderr'
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 100MB
log_min_duration_statement = 1000       # Log queries > 1 second
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on
log_temp_files = 0

#------------------------------------------------------------------------------
# EXTENSIONS
#------------------------------------------------------------------------------
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all
```

### 7.2 Docker Compose Configuration

```yaml
# docker-compose.yml - PostgreSQL 18 optimized
services:
  postgres:
    image: pgvector/pgvector:pg18
    container_name: secondbrain-postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=${POSTGRES_DB:-secondbrain}
    command:
      - "postgres"
      # Memory
      - "-c"
      - "shared_buffers=1GB"
      - "-c"
      - "effective_cache_size=3GB"
      - "-c"
      - "work_mem=64MB"
      - "-c"
      - "maintenance_work_mem=512MB"
      # Async I/O
      - "-c"
      - "io_method=worker"
      - "-c"
      - "io_workers=4"
      - "-c"
      - "effective_io_concurrency=200"
      # Parallel Query
      - "-c"
      - "max_parallel_workers_per_gather=2"
      - "-c"
      - "max_parallel_workers=4"
      - "-c"
      - "max_parallel_maintenance_workers=2"
      # SSD Optimization
      - "-c"
      - "random_page_cost=1.1"
      # Monitoring
      - "-c"
      - "shared_preload_libraries=pg_stat_statements"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/00_extensions.sql:/docker-entrypoint-initdb.d/00_extensions.sql:ro
      # ... other init scripts
    ports:
      - "5432:5432"
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
```

### 7.3 Index Optimization Strategy

```sql
-- Analyze current index usage
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan AS index_scans,
    idx_tup_read AS tuples_read,
    idx_tup_fetch AS tuples_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;

-- Find unused indexes (candidates for removal)
SELECT 
    schemaname || '.' || relname AS table,
    indexrelname AS index,
    pg_size_pretty(pg_relation_size(i.indexrelid)) AS index_size,
    idx_scan as index_scans
FROM pg_stat_user_indexes ui
JOIN pg_index i ON ui.indexrelid = i.indexrelid
WHERE NOT indisunique  -- Keep unique indexes
  AND idx_scan < 50    -- Low usage threshold
ORDER BY pg_relation_size(i.indexrelid) DESC;

-- Find missing indexes (high sequential scans)
SELECT 
    schemaname || '.' || relname AS table,
    seq_scan,
    seq_tup_read,
    idx_scan,
    n_live_tup AS row_count,
    seq_scan - idx_scan AS seq_vs_idx_diff
FROM pg_stat_user_tables
WHERE seq_scan > idx_scan
  AND n_live_tup > 10000
ORDER BY seq_tup_read DESC
LIMIT 20;
```

---

## 8. Monitoring & Observability

### 8.1 pg_stat_statements Improvements

#### Enable Extension

```sql
-- Enable pg_stat_statements
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Reset statistics (useful after changes)
SELECT pg_stat_statements_reset();
```

#### PostgreSQL 18 New Metrics

```sql
-- Query performance with new PG18 metrics
SELECT 
    queryid,
    LEFT(query, 100) AS query_preview,
    calls,
    round(total_exec_time::numeric, 2) AS total_time_ms,
    round(mean_exec_time::numeric, 2) AS avg_time_ms,
    rows,
    -- New in PostgreSQL 18
    parallel_workers_to_launch,
    parallel_workers_launched,
    wal_buffers_full
FROM pg_stat_statements
WHERE calls > 10
ORDER BY total_exec_time DESC
LIMIT 20;

-- Identify queries with parallelism issues
SELECT 
    queryid,
    LEFT(query, 100) AS query_preview,
    calls,
    parallel_workers_to_launch,
    parallel_workers_launched,
    parallel_workers_to_launch - parallel_workers_launched AS workers_not_launched
FROM pg_stat_statements
WHERE parallel_workers_to_launch > 0
  AND parallel_workers_to_launch > parallel_workers_launched
ORDER BY (parallel_workers_to_launch - parallel_workers_launched) DESC
LIMIT 10;
```

### 8.2 RAG-Specific Monitoring Queries

```sql
-- Vector search performance
SELECT 
    queryid,
    LEFT(query, 150) AS query_preview,
    calls,
    round(mean_exec_time::numeric, 2) AS avg_ms,
    rows / NULLIF(calls, 0) AS avg_rows
FROM pg_stat_statements
WHERE query LIKE '%embedding%<=>%'
   OR query LIKE '%vector%'
ORDER BY total_exec_time DESC
LIMIT 10;

-- Full-text search performance
SELECT 
    queryid,
    LEFT(query, 150) AS query_preview,
    calls,
    round(mean_exec_time::numeric, 2) AS avg_ms
FROM pg_stat_statements
WHERE query LIKE '%search_vector%@@%'
   OR query LIKE '%ts_rank%'
ORDER BY total_exec_time DESC
LIMIT 10;

-- Combined RAG analytics
SELECT 
    date_trunc('hour', created_at) AS hour,
    COUNT(*) AS queries,
    AVG(total_time_ms) AS avg_total_ms,
    AVG(vector_search_time_ms) AS avg_vector_ms,
    AVG(bm25_search_time_ms) AS avg_bm25_ms,
    AVG(rerank_time_ms) AS avg_rerank_ms,
    SUM(CASE WHEN user_feedback = 'thumbs_up' THEN 1 ELSE 0 END)::float / 
        NULLIF(SUM(CASE WHEN user_feedback IS NOT NULL THEN 1 ELSE 0 END), 0) AS positive_rate
FROM rag_query_logs
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY date_trunc('hour', created_at)
ORDER BY hour DESC;
```

### 8.3 Async I/O Monitoring

```sql
-- Monitor async I/O activity
SELECT * FROM pg_aios;

-- I/O statistics by backend type
SELECT 
    backend_type,
    object,
    context,
    reads,
    read_time,
    writes,
    write_time
FROM pg_stat_io
WHERE reads > 0 OR writes > 0
ORDER BY read_time + write_time DESC;
```

### 8.4 Connection Pooling Monitoring

```sql
-- Active connections by state
SELECT 
    state,
    usename,
    application_name,
    COUNT(*) as connection_count,
    MAX(NOW() - backend_start) as oldest_connection
FROM pg_stat_activity
WHERE backend_type = 'client backend'
GROUP BY state, usename, application_name
ORDER BY connection_count DESC;

-- Long-running queries
SELECT 
    pid,
    usename,
    application_name,
    state,
    NOW() - query_start AS duration,
    LEFT(query, 100) AS query_preview
FROM pg_stat_activity
WHERE state != 'idle'
  AND query_start < NOW() - INTERVAL '30 seconds'
ORDER BY duration DESC;
```

### 8.5 Monitoring Dashboard Queries

```sql
-- Create a monitoring function for Second Brain
CREATE OR REPLACE FUNCTION get_secondbrain_stats()
RETURNS TABLE (
    metric_name TEXT,
    metric_value TEXT
) AS $$
BEGIN
    -- Total notes
    RETURN QUERY SELECT 'total_notes'::TEXT, COUNT(*)::TEXT FROM notes WHERE NOT is_deleted;
    
    -- Total embeddings
    RETURN QUERY SELECT 'total_embeddings'::TEXT, COUNT(*)::TEXT FROM note_embeddings;
    
    -- Total conversations
    RETURN QUERY SELECT 'total_conversations'::TEXT, COUNT(*)::TEXT FROM chat_conversations WHERE NOT is_deleted;
    
    -- RAG queries today
    RETURN QUERY SELECT 'rag_queries_today'::TEXT, COUNT(*)::TEXT 
                 FROM rag_query_logs 
                 WHERE created_at > CURRENT_DATE;
    
    -- Positive feedback rate
    RETURN QUERY SELECT 'positive_feedback_rate'::TEXT, 
                 ROUND(100.0 * SUM(CASE WHEN user_feedback = 'thumbs_up' THEN 1 ELSE 0 END) /
                       NULLIF(COUNT(*), 0), 1)::TEXT || '%'
                 FROM rag_query_logs
                 WHERE user_feedback IS NOT NULL
                   AND created_at > NOW() - INTERVAL '7 days';
    
    -- Database size
    RETURN QUERY SELECT 'database_size'::TEXT, pg_size_pretty(pg_database_size(current_database()));
    
    -- Embedding index size
    RETURN QUERY SELECT 'embedding_index_size'::TEXT, 
                 pg_size_pretty(pg_relation_size('note_embeddings'));
END;
$$ LANGUAGE plpgsql;

-- Usage
SELECT * FROM get_secondbrain_stats();
```

---

## 9. Implementation Roadmap

### Phase 1: Configuration (Low Risk) - Week 1

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Enable Async I/O settings in docker-compose.yml | High | 1 hour | 2-3x scan performance |
| Configure parallel index builds | High | 30 min | 50-70% faster index creation |
| Enable pg_stat_statements | Medium | 30 min | Better observability |
| Optimize PostgreSQL memory settings | Medium | 1 hour | Overall performance |

**Deliverables:**

- Updated `docker-compose.yml` with PG18 optimizations
- Updated PostgreSQL configuration
- Monitoring queries documented

### Phase 2: Schema Enhancements (Medium Risk) - Week 2-3

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Add UUIDv7 support for new tables | High | 4 hours | 4x insert performance |
| Convert search_vector to generated column | Medium | 2 hours | Reduced trigger overhead |
| Upgrade pgvector to 0.8.x | High | 2 hours | Better filtered queries |
| Optimize HNSW index parameters | Medium | 2 hours | Better recall/speed |

**Deliverables:**

- Migration script `13_postgresql_18_features.sql`
- Updated EF Core configuration
- pgvector upgrade procedure

### Phase 3: Code Updates (Medium Risk) - Week 4-5

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Implement MERGE-based upsert | High | 4 hours | Atomic operations |
| Replace in-memory BM25 with native SQL | High | 8 hours | Better hybrid search |
| Add JSON_TABLE queries for analytics | Medium | 4 hours | Cleaner JSON handling |
| Update Npgsql to 10.x | High | 4 hours | UUIDv7 support |

**Deliverables:**

- Updated `PostgresVectorStore.cs`
- Updated `BM25SearchService.cs`
- New analytics queries

### Phase 4: Advanced Features (Higher Risk) - Week 6-8

| Task | Priority | Effort | Impact |
|------|----------|--------|--------|
| Implement note version history with temporal constraints | Medium | 8 hours | Full audit trail |
| Migrate existing PKs to UUIDv7 | Low | 16 hours | Long-term performance |
| Add session tracking with temporal tables | Low | 4 hours | Usage analytics |

**Deliverables:**

- Note versioning system
- Data migration scripts
- Session tracking tables

---

## 10. Migration Scripts

### 10.1 Initial Setup Script

Create `database/13_postgresql_18_features.sql`:

```sql
-- ============================================================================
-- Second Brain Database - PostgreSQL 18 Features Migration
-- ============================================================================
-- This script enables PostgreSQL 18 features for improved performance
-- Run after upgrading to PostgreSQL 18
-- ============================================================================

-- Check PostgreSQL version
DO $$
BEGIN
    IF current_setting('server_version_num')::int < 180000 THEN
        RAISE EXCEPTION 'PostgreSQL 18 or higher required. Current version: %', 
            current_setting('server_version');
    END IF;
END $$;

\echo 'PostgreSQL 18 detected, proceeding with feature enablement...'

-- ============================================================================
-- 1. Enable pg_stat_statements for monitoring
-- ============================================================================
\echo 'Step 1: Enabling pg_stat_statements...'

CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- ============================================================================
-- 2. Update pgvector extension
-- ============================================================================
\echo 'Step 2: Updating pgvector extension...'

-- Update to latest version if available
DO $$
BEGIN
    IF EXISTS (SELECT 1 FROM pg_extension WHERE extname = 'vector') THEN
        ALTER EXTENSION vector UPDATE;
        RAISE NOTICE 'pgvector extension updated';
    END IF;
END $$;

-- ============================================================================
-- 3. Add UUIDv7 defaults for new records
-- ============================================================================
\echo 'Step 3: Adding UUIDv7 support...'

-- Add uuid_v7 columns to existing tables (non-breaking)
ALTER TABLE notes ADD COLUMN IF NOT EXISTS uuid_v7 UUID DEFAULT uuidv7();
ALTER TABLE note_embeddings ADD COLUMN IF NOT EXISTS uuid_v7 UUID DEFAULT uuidv7();
ALTER TABLE chat_conversations ADD COLUMN IF NOT EXISTS uuid_v7 UUID DEFAULT uuidv7();
ALTER TABLE chat_messages ADD COLUMN IF NOT EXISTS uuid_v7 UUID DEFAULT uuidv7();

-- Create indexes on new UUID columns
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_notes_uuid_v7 ON notes(uuid_v7);
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_embeddings_uuid_v7 ON note_embeddings(uuid_v7);
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_conversations_uuid_v7 ON chat_conversations(uuid_v7);
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_messages_uuid_v7 ON chat_messages(uuid_v7);

-- ============================================================================
-- 4. Convert search_vector to generated column
-- ============================================================================
\echo 'Step 4: Converting search_vector to generated column...'

-- Drop existing trigger
DROP TRIGGER IF EXISTS trg_note_embedding_search_vector ON note_embeddings;
DROP FUNCTION IF EXISTS update_note_embedding_search_vector();

-- Note: We cannot directly alter to a generated column, need to recreate
-- For existing data, we keep the existing column as-is
-- New approach: Use a view or compute on-demand

-- Create a function for search vector generation (fallback)
CREATE OR REPLACE FUNCTION compute_search_vector(title TEXT, content TEXT)
RETURNS tsvector AS $$
BEGIN
    RETURN setweight(to_tsvector('english', COALESCE(title, '')), 'A') ||
           setweight(to_tsvector('english', COALESCE(content, '')), 'B');
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Ensure search_vector is populated for all records
UPDATE note_embeddings
SET search_vector = compute_search_vector(note_title, content)
WHERE search_vector IS NULL;

-- ============================================================================
-- 5. Optimize HNSW index for pgvector 0.8
-- ============================================================================
\echo 'Step 5: Optimizing vector indexes...'

-- Drop old index if exists
DROP INDEX CONCURRENTLY IF EXISTS ix_embeddings_vector;
DROP INDEX CONCURRENTLY IF EXISTS ix_embeddings_hnsw;

-- Create optimized HNSW index
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_embeddings_hnsw
ON note_embeddings USING hnsw (embedding vector_cosine_ops)
WITH (m = 24, ef_construction = 128);

-- ============================================================================
-- 6. Add parallel GIN index for full-text search
-- ============================================================================
\echo 'Step 6: Recreating GIN index with parallel build...'

-- Set parallel workers for index build
SET max_parallel_maintenance_workers = 4;

-- Recreate GIN index
DROP INDEX CONCURRENTLY IF EXISTS idx_note_embeddings_search_vector;
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_note_embeddings_search_vector
ON note_embeddings USING GIN(search_vector);

-- ============================================================================
-- 7. Add performance indexes for skip scan optimization
-- ============================================================================
\echo 'Step 7: Adding skip scan optimized indexes...'

-- Indexes designed to benefit from skip scan
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_embeddings_provider_user_note
ON note_embeddings(embedding_provider, user_id, note_id);

CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_conversations_provider_model
ON chat_conversations(provider, model, user_id);

-- ============================================================================
-- 8. Add JSONB columns for better JSON handling
-- ============================================================================
\echo 'Step 8: Adding JSONB columns...'

-- Add JSONB columns to tool_calls
ALTER TABLE tool_calls 
    ADD COLUMN IF NOT EXISTS arguments_jsonb JSONB,
    ADD COLUMN IF NOT EXISTS result_jsonb JSONB;

-- Migrate existing JSON text to JSONB
UPDATE tool_calls 
SET 
    arguments_jsonb = CASE 
        WHEN arguments IS NOT NULL AND arguments != '' 
        THEN arguments::jsonb 
        ELSE '{}'::jsonb 
    END,
    result_jsonb = CASE 
        WHEN result IS NOT NULL AND result != '' 
        THEN result::jsonb 
        ELSE '{}'::jsonb 
    END
WHERE arguments_jsonb IS NULL OR result_jsonb IS NULL;

-- Add GIN indexes for JSONB columns
CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_tool_calls_arguments_gin
ON tool_calls USING GIN(arguments_jsonb jsonb_path_ops);

CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_tool_calls_result_gin
ON tool_calls USING GIN(result_jsonb jsonb_path_ops);

-- ============================================================================
-- 9. Create monitoring function
-- ============================================================================
\echo 'Step 9: Creating monitoring functions...'

CREATE OR REPLACE FUNCTION get_pg18_feature_status()
RETURNS TABLE (
    feature_name TEXT,
    status TEXT,
    details TEXT
) AS $$
BEGIN
    -- Check Async I/O
    RETURN QUERY SELECT 
        'Async I/O'::TEXT,
        current_setting('io_method', true),
        'io_workers: ' || current_setting('io_workers', true);
    
    -- Check pgvector version
    RETURN QUERY SELECT 
        'pgvector'::TEXT,
        (SELECT extversion FROM pg_extension WHERE extname = 'vector'),
        'HNSW index exists: ' || EXISTS(
            SELECT 1 FROM pg_indexes 
            WHERE indexname = 'ix_embeddings_hnsw'
        )::TEXT;
    
    -- Check pg_stat_statements
    RETURN QUERY SELECT 
        'pg_stat_statements'::TEXT,
        CASE WHEN EXISTS(SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements')
            THEN 'enabled' ELSE 'disabled' END,
        '';
    
    -- Check UUIDv7 columns
    RETURN QUERY SELECT 
        'UUIDv7 columns'::TEXT,
        CASE WHEN EXISTS(
            SELECT 1 FROM information_schema.columns 
            WHERE table_name = 'notes' AND column_name = 'uuid_v7'
        ) THEN 'added' ELSE 'not added' END,
        '';
    
    -- Check JSONB columns
    RETURN QUERY SELECT 
        'JSONB tool_calls'::TEXT,
        CASE WHEN EXISTS(
            SELECT 1 FROM information_schema.columns 
            WHERE table_name = 'tool_calls' AND column_name = 'arguments_jsonb'
        ) THEN 'migrated' ELSE 'not migrated' END,
        '';
END;
$$ LANGUAGE plpgsql;

-- ============================================================================
-- 10. Update table comments
-- ============================================================================
\echo 'Step 10: Updating documentation...'

COMMENT ON COLUMN notes.uuid_v7 IS 'UUIDv7 identifier for improved index performance (PostgreSQL 18)';
COMMENT ON COLUMN note_embeddings.uuid_v7 IS 'UUIDv7 identifier for improved index performance (PostgreSQL 18)';
COMMENT ON COLUMN tool_calls.arguments_jsonb IS 'JSONB version of arguments for efficient querying (PostgreSQL 18)';
COMMENT ON COLUMN tool_calls.result_jsonb IS 'JSONB version of result for efficient querying (PostgreSQL 18)';
COMMENT ON INDEX ix_embeddings_hnsw IS 'Optimized HNSW index for pgvector 0.8 with m=24, ef_construction=128';

-- ============================================================================
-- Complete
-- ============================================================================
\echo ''
\echo '============================================'
\echo 'PostgreSQL 18 features migration complete!'
\echo '============================================'
\echo ''
\echo 'Feature status:'
SELECT * FROM get_pg18_feature_status();
\echo ''
\echo 'Next steps:'
\echo '  1. Update docker-compose.yml with io_method and io_workers settings'
\echo '  2. Update Npgsql to version 10.x for UUIDv7 support'
\echo '  3. Update backend code to use MERGE for upserts'
\echo '  4. Update BM25SearchService to use native PostgreSQL full-text search'
\echo '============================================'
```

### 10.2 Rollback Script

Create `database/13_postgresql_18_features_rollback.sql`:

```sql
-- ============================================================================
-- Second Brain Database - PostgreSQL 18 Features Rollback
-- ============================================================================
-- Use this script to rollback PostgreSQL 18 feature changes if needed
-- ============================================================================

\echo 'Rolling back PostgreSQL 18 features...'

-- Drop UUIDv7 columns and indexes
DROP INDEX CONCURRENTLY IF EXISTS ix_notes_uuid_v7;
DROP INDEX CONCURRENTLY IF EXISTS ix_embeddings_uuid_v7;
DROP INDEX CONCURRENTLY IF EXISTS ix_conversations_uuid_v7;
DROP INDEX CONCURRENTLY IF EXISTS ix_messages_uuid_v7;

ALTER TABLE notes DROP COLUMN IF EXISTS uuid_v7;
ALTER TABLE note_embeddings DROP COLUMN IF EXISTS uuid_v7;
ALTER TABLE chat_conversations DROP COLUMN IF EXISTS uuid_v7;
ALTER TABLE chat_messages DROP COLUMN IF EXISTS uuid_v7;

-- Drop skip scan optimized indexes
DROP INDEX CONCURRENTLY IF EXISTS ix_embeddings_provider_user_note;
DROP INDEX CONCURRENTLY IF EXISTS ix_conversations_provider_model;

-- Drop JSONB indexes and columns
DROP INDEX CONCURRENTLY IF EXISTS ix_tool_calls_arguments_gin;
DROP INDEX CONCURRENTLY IF EXISTS ix_tool_calls_result_gin;
ALTER TABLE tool_calls DROP COLUMN IF EXISTS arguments_jsonb;
ALTER TABLE tool_calls DROP COLUMN IF EXISTS result_jsonb;

-- Restore original HNSW index
DROP INDEX CONCURRENTLY IF EXISTS ix_embeddings_hnsw;

-- Recreate original trigger for search_vector
CREATE OR REPLACE FUNCTION update_note_embedding_search_vector()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector := 
        setweight(to_tsvector('english', COALESCE(NEW.note_title, '')), 'A') ||
        setweight(to_tsvector('english', COALESCE(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS trg_note_embedding_search_vector ON note_embeddings;
CREATE TRIGGER trg_note_embedding_search_vector
    BEFORE INSERT OR UPDATE OF note_title, content ON note_embeddings
    FOR EACH ROW
    EXECUTE FUNCTION update_note_embedding_search_vector();

-- Drop monitoring function
DROP FUNCTION IF EXISTS get_pg18_feature_status();
DROP FUNCTION IF EXISTS compute_search_vector(TEXT, TEXT);

\echo 'Rollback complete.'
```

---

## 11. Backend Code Updates

### 11.1 ApplicationDbContext Updates

```csharp
// backend/src/SecondBrain.Infrastructure/Data/ApplicationDbContext.cs

using Microsoft.EntityFrameworkCore;
using Npgsql;

public class ApplicationDbContext : DbContext
{
    protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder)
    {
        // Configure for PostgreSQL 18
        optionsBuilder.UseNpgsql(connectionString, npgsqlOptions =>
        {
            npgsqlOptions.SetPostgresVersion(18, 0);
            npgsqlOptions.EnableRetryOnFailure(3);
        });
    }

    protected override void OnModelCreating(ModelBuilder modelBuilder)
    {
        base.OnModelCreating(modelBuilder);

        // Configure UUIDv7 for new entities
        modelBuilder.Entity<Note>(entity =>
        {
            // Use UUIDv7 for new records
            entity.Property(e => e.UuidV7)
                .HasDefaultValueSql("uuidv7()");
        });

        modelBuilder.Entity<NoteEmbedding>(entity =>
        {
            entity.Property(e => e.UuidV7)
                .HasDefaultValueSql("uuidv7()");
                
            // Configure JSONB for tool calls (when migrated)
            // entity.Property(e => e.ArgumentsJsonb)
            //     .HasColumnType("jsonb");
        });
    }
}
```

### 11.2 PostgresVectorStore with MERGE

```csharp
// backend/src/SecondBrain.Infrastructure/VectorStore/PostgresVectorStore.cs

public class PostgresVectorStore : IVectorStore
{
    /// <summary>
    /// Upsert embedding using PostgreSQL 18 MERGE statement
    /// </summary>
    public async Task<UpsertResult> UpsertWithMergeAsync(
        NoteEmbedding embedding,
        CancellationToken cancellationToken = default)
    {
        try
        {
            var sql = @"
                MERGE INTO note_embeddings AS target
                USING (SELECT 
                    @id AS id, 
                    @noteId AS note_id, 
                    @userId AS user_id,
                    @chunkIndex AS chunk_index, 
                    @content AS content, 
                    @embedding AS embedding,
                    @provider AS embedding_provider, 
                    @model AS embedding_model, 
                    @noteTitle AS note_title,
                    @noteTags AS note_tags, 
                    @noteUpdatedAt AS note_updated_at
                ) AS source
                ON target.id = source.id
                WHEN MATCHED THEN
                    UPDATE SET
                        note_id = source.note_id,
                        user_id = source.user_id,
                        chunk_index = source.chunk_index,
                        content = source.content,
                        embedding = source.embedding,
                        embedding_provider = source.embedding_provider,
                        embedding_model = source.embedding_model,
                        note_title = source.note_title,
                        note_tags = source.note_tags,
                        note_updated_at = source.note_updated_at,
                        search_vector = setweight(to_tsvector('english', COALESCE(source.note_title, '')), 'A') ||
                                       setweight(to_tsvector('english', COALESCE(source.content, '')), 'B')
                WHEN NOT MATCHED THEN
                    INSERT (id, note_id, user_id, chunk_index, content, embedding,
                            embedding_provider, embedding_model, note_title, note_tags,
                            note_updated_at, created_at, search_vector)
                    VALUES (source.id, source.note_id, source.user_id, source.chunk_index,
                            source.content, source.embedding, source.embedding_provider,
                            source.embedding_model, source.note_title, source.note_tags,
                            source.note_updated_at, NOW(),
                            setweight(to_tsvector('english', COALESCE(source.note_title, '')), 'A') ||
                            setweight(to_tsvector('english', COALESCE(source.content, '')), 'B'))
                RETURNING merge_action() AS action, id;";

            var parameters = new[]
            {
                new NpgsqlParameter("@id", embedding.Id ?? Guid.NewGuid().ToString()),
                new NpgsqlParameter("@noteId", embedding.NoteId),
                new NpgsqlParameter("@userId", embedding.UserId),
                new NpgsqlParameter("@chunkIndex", embedding.ChunkIndex),
                new NpgsqlParameter("@content", embedding.Content),
                new NpgsqlParameter("@embedding", embedding.Embedding),
                new NpgsqlParameter("@provider", embedding.EmbeddingProvider),
                new NpgsqlParameter("@model", embedding.EmbeddingModel),
                new NpgsqlParameter("@noteTitle", embedding.NoteTitle),
                new NpgsqlParameter("@noteTags", embedding.NoteTags ?? Array.Empty<string>()),
                new NpgsqlParameter("@noteUpdatedAt", embedding.NoteUpdatedAt)
            };

            var connection = _context.Database.GetDbConnection();
            await using var command = connection.CreateCommand();
            command.CommandText = sql;
            command.Parameters.AddRange(parameters);

            if (connection.State != System.Data.ConnectionState.Open)
                await connection.OpenAsync(cancellationToken);

            await using var reader = await command.ExecuteReaderAsync(cancellationToken);
            
            if (await reader.ReadAsync(cancellationToken))
            {
                var action = reader.GetString(0);
                var id = reader.GetString(1);
                return new UpsertResult(true, action == "INSERT", id);
            }

            return new UpsertResult(false, false, null);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "MERGE upsert failed for embedding {Id}", embedding.Id);
            return new UpsertResult(false, false, null);
        }
    }

    public record UpsertResult(bool Success, bool WasInsert, string? Id);
}
```

### 11.3 Native BM25 Search Service

```csharp
// backend/src/SecondBrain.Application/Services/RAG/NativeBM25SearchService.cs

using Npgsql;
using NpgsqlTypes;

namespace SecondBrain.Application.Services.RAG;

/// <summary>
/// BM25 search using native PostgreSQL full-text search (PostgreSQL 18 optimized)
/// </summary>
public class NativeBM25SearchService : IBM25SearchService
{
    private readonly ApplicationDbContext _context;
    private readonly ILogger<NativeBM25SearchService> _logger;

    public NativeBM25SearchService(
        ApplicationDbContext context,
        ILogger<NativeBM25SearchService> logger)
    {
        _context = context;
        _logger = logger;
    }

    public async Task<List<BM25SearchResult>> SearchAsync(
        string query,
        string userId,
        int topK,
        CancellationToken cancellationToken = default)
    {
        if (string.IsNullOrWhiteSpace(query))
            return new List<BM25SearchResult>();

        try
        {
            _logger.LogInformation(
                "Starting native BM25 search. UserId: {UserId}, Query: {Query}, TopK: {TopK}",
                userId, query, topK);

            // Use ts_rank_cd for cover density ranking (BM25-like)
            var sql = @"
                SELECT 
                    ne.id,
                    ne.note_id,
                    ne.content,
                    ne.note_title,
                    ne.note_tags,
                    ne.chunk_index,
                    ts_rank_cd(
                        ne.search_vector,
                        plainto_tsquery('english', @query),
                        32  -- Normalize by document length
                    ) AS bm25_score,
                    ROW_NUMBER() OVER (
                        ORDER BY ts_rank_cd(ne.search_vector, plainto_tsquery('english', @query), 32) DESC
                    ) AS rank
                FROM note_embeddings ne
                WHERE ne.user_id = @userId
                  AND ne.search_vector @@ plainto_tsquery('english', @query)
                ORDER BY bm25_score DESC
                LIMIT @topK";

            var connection = _context.Database.GetDbConnection();
            await using var command = connection.CreateCommand();
            command.CommandText = sql;
            command.Parameters.Add(new NpgsqlParameter("@query", query));
            command.Parameters.Add(new NpgsqlParameter("@userId", userId));
            command.Parameters.Add(new NpgsqlParameter("@topK", topK));

            if (connection.State != System.Data.ConnectionState.Open)
                await connection.OpenAsync(cancellationToken);

            var results = new List<BM25SearchResult>();
            await using var reader = await command.ExecuteReaderAsync(cancellationToken);

            while (await reader.ReadAsync(cancellationToken))
            {
                results.Add(new BM25SearchResult
                {
                    Id = reader.GetString(0),
                    NoteId = reader.GetString(1),
                    Content = reader.GetString(2),
                    NoteTitle = reader.GetString(3),
                    NoteTags = reader.GetFieldValue<List<string>>(4),
                    ChunkIndex = reader.GetInt32(5),
                    BM25Score = reader.GetFloat(6),
                    Rank = reader.GetInt32(7)
                });
            }

            _logger.LogInformation(
                "Native BM25 search completed. UserId: {UserId}, Results: {ResultCount}",
                userId, results.Count);

            return results;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Native BM25 search failed. UserId: {UserId}", userId);
            return new List<BM25SearchResult>();
        }
    }

    /// <summary>
    /// Search with highlighted snippets
    /// </summary>
    public async Task<List<BM25SearchResultWithHighlight>> SearchWithHighlightAsync(
        string query,
        string userId,
        int topK,
        CancellationToken cancellationToken = default)
    {
        var sql = @"
            SELECT 
                ne.id,
                ne.note_id,
                ne.note_title,
                ts_rank_cd(ne.search_vector, query, 32) AS score,
                ts_headline(
                    'english',
                    ne.content,
                    query,
                    'StartSel=<mark>, StopSel=</mark>, MaxWords=50, MinWords=25, MaxFragments=3'
                ) AS highlighted_content
            FROM note_embeddings ne,
                 plainto_tsquery('english', @query) query
            WHERE ne.user_id = @userId
              AND ne.search_vector @@ query
            ORDER BY score DESC
            LIMIT @topK";

        // Execute and map results...
        throw new NotImplementedException("Add implementation");
    }
}

public class BM25SearchResultWithHighlight : BM25SearchResult
{
    public string HighlightedContent { get; set; } = string.Empty;
}
```

### 11.4 Hybrid Search with Native SQL

```csharp
// backend/src/SecondBrain.Application/Services/RAG/NativeHybridSearchService.cs

/// <summary>
/// Hybrid search using native PostgreSQL 18 features
/// Combines vector search and full-text search using Reciprocal Rank Fusion
/// </summary>
public class NativeHybridSearchService : IHybridSearchService
{
    public async Task<List<HybridSearchResult>> SearchAsync(
        string query,
        List<double> queryEmbedding,
        string userId,
        int topK,
        float similarityThreshold = 0.3f,
        CancellationToken cancellationToken = default)
    {
        var sql = @"
            WITH vector_results AS (
                SELECT 
                    id,
                    note_id,
                    content,
                    note_title,
                    note_tags,
                    chunk_index,
                    1 - (embedding <=> @queryEmbedding::vector) AS vector_score,
                    ROW_NUMBER() OVER (ORDER BY embedding <=> @queryEmbedding::vector) AS vector_rank
                FROM note_embeddings
                WHERE user_id = @userId
                  AND embedding IS NOT NULL
                ORDER BY embedding <=> @queryEmbedding::vector
                LIMIT @initialK
            ),
            bm25_results AS (
                SELECT 
                    id,
                    note_id,
                    content,
                    note_title,
                    note_tags,
                    chunk_index,
                    ts_rank_cd(search_vector, plainto_tsquery('english', @query), 32) AS bm25_score,
                    ROW_NUMBER() OVER (
                        ORDER BY ts_rank_cd(search_vector, plainto_tsquery('english', @query), 32) DESC
                    ) AS bm25_rank
                FROM note_embeddings
                WHERE user_id = @userId
                  AND search_vector @@ plainto_tsquery('english', @query)
                LIMIT @initialK
            )
            SELECT 
                COALESCE(v.id, b.id) AS id,
                COALESCE(v.note_id, b.note_id) AS note_id,
                COALESCE(v.content, b.content) AS content,
                COALESCE(v.note_title, b.note_title) AS note_title,
                COALESCE(v.note_tags, b.note_tags) AS note_tags,
                COALESCE(v.chunk_index, b.chunk_index) AS chunk_index,
                COALESCE(v.vector_score, 0) AS vector_score,
                COALESCE(b.bm25_score, 0) AS bm25_score,
                COALESCE(v.vector_rank, 1000) AS vector_rank,
                COALESCE(b.bm25_rank, 1000) AS bm25_rank,
                -- RRF Score calculation
                (COALESCE(1.0 / (60 + v.vector_rank), 0) * @vectorWeight +
                 COALESCE(1.0 / (60 + b.bm25_rank), 0) * @bm25Weight) AS rrf_score,
                v.id IS NOT NULL AS found_in_vector,
                b.id IS NOT NULL AS found_in_bm25
            FROM vector_results v
            FULL OUTER JOIN bm25_results b ON v.id = b.id
            ORDER BY rrf_score DESC
            LIMIT @topK";

        // Execute query with parameters
        // Map results to HybridSearchResult
        throw new NotImplementedException("Add full implementation");
    }
}
```

---

## 12. Testing & Validation

### 12.1 Feature Verification Queries

```sql
-- Verify PostgreSQL 18 is running
SELECT version();

-- Check enabled features
SELECT * FROM get_pg18_feature_status();

-- Verify pgvector version
SELECT extversion FROM pg_extension WHERE extname = 'vector';

-- Verify UUIDv7 function exists
SELECT uuidv7();

-- Verify HNSW index
SELECT indexname, indexdef 
FROM pg_indexes 
WHERE indexname = 'ix_embeddings_hnsw';

-- Verify GIN index
SELECT indexname, indexdef 
FROM pg_indexes 
WHERE indexname = 'idx_note_embeddings_search_vector';

-- Verify Async I/O configuration
SHOW io_method;
SHOW io_workers;
SHOW effective_io_concurrency;
```

### 12.2 Performance Benchmarks

```sql
-- Vector search benchmark
EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
SELECT id, note_title, 1 - (embedding <=> @test_vector::vector) AS score
FROM note_embeddings
WHERE user_id = 'test-user'
ORDER BY embedding <=> @test_vector::vector
LIMIT 10;

-- Full-text search benchmark
EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
SELECT id, note_title, ts_rank_cd(search_vector, query) AS score
FROM note_embeddings, plainto_tsquery('english', 'machine learning') query
WHERE user_id = 'test-user'
  AND search_vector @@ query
ORDER BY score DESC
LIMIT 10;

-- Hybrid search benchmark
EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)
WITH vector_results AS (
    SELECT id, ROW_NUMBER() OVER (ORDER BY embedding <=> @test_vector::vector) AS vrank
    FROM note_embeddings WHERE user_id = 'test-user' LIMIT 20
),
bm25_results AS (
    SELECT id, ROW_NUMBER() OVER (ORDER BY ts_rank_cd(search_vector, query) DESC) AS brank
    FROM note_embeddings, plainto_tsquery('english', 'test query') query
    WHERE user_id = 'test-user' AND search_vector @@ query LIMIT 20
)
SELECT COALESCE(v.id, b.id),
       1.0/(60+COALESCE(v.vrank, 1000)) + 1.0/(60+COALESCE(b.brank, 1000)) AS rrf
FROM vector_results v FULL OUTER JOIN bm25_results b ON v.id = b.id
ORDER BY rrf DESC LIMIT 10;
```

### 12.3 Integration Test Checklist

| Test Case | Status | Notes |
|-----------|--------|-------|
| UUIDv7 generation works | | `SELECT uuidv7();` |
| New notes get UUIDv7 | | Insert and verify uuid_v7 column |
| Vector search returns results | | Test with sample embedding |
| BM25 search returns results | | Test with sample query |
| Hybrid search combines results | | Verify RRF scores |
| MERGE upsert works | | Test insert and update |
| pg_stat_statements tracks queries | | Check after running queries |
| Async I/O is active | | Check pg_aios view |

### 12.4 Load Testing Queries

```sql
-- Generate test embeddings (for load testing)
INSERT INTO note_embeddings (id, note_id, user_id, chunk_index, content, 
                             embedding, embedding_provider, embedding_model,
                             note_title, note_tags, note_updated_at, created_at)
SELECT 
    'test-' || generate_series,
    'note-' || (generate_series % 100),
    'user-' || (generate_series % 10),
    generate_series % 5,
    'Test content for embedding ' || generate_series,
    (SELECT array_agg(random())::vector(1536) FROM generate_series(1, 1536)),
    'test-provider',
    'test-model',
    'Test Note ' || generate_series,
    ARRAY['test', 'benchmark'],
    NOW(),
    NOW()
FROM generate_series(1, 10000);

-- Measure search performance at scale
\timing on

SELECT COUNT(*) FROM note_embeddings;

-- Vector search at 10K embeddings
SELECT id, note_title
FROM note_embeddings
ORDER BY embedding <=> (SELECT embedding FROM note_embeddings LIMIT 1)
LIMIT 10;

-- Full-text search at 10K embeddings
SELECT id, note_title
FROM note_embeddings
WHERE search_vector @@ plainto_tsquery('english', 'test content')
ORDER BY ts_rank_cd(search_vector, plainto_tsquery('english', 'test content')) DESC
LIMIT 10;

\timing off

-- Clean up test data
DELETE FROM note_embeddings WHERE id LIKE 'test-%';
```

---

## Appendix A: Quick Reference Card

### PostgreSQL 18 New SQL Syntax

```sql
-- UUIDv7 generation
SELECT uuidv7();

-- MERGE with RETURNING
MERGE INTO target USING source ON condition
WHEN MATCHED THEN UPDATE SET ...
WHEN NOT MATCHED THEN INSERT ...
RETURNING merge_action(), OLD.col, NEW.col;

-- JSON_TABLE
SELECT * FROM JSON_TABLE(
    json_column,
    '$.path'
    COLUMNS (col1 TYPE PATH '$.field1', col2 TYPE PATH '$.field2')
) AS jt;

-- Temporal constraint
PRIMARY KEY (id, period WITHOUT OVERLAPS)

-- Virtual generated column
column_name TYPE GENERATED ALWAYS AS (expression) VIRTUAL

-- Stored generated column
column_name TYPE GENERATED ALWAYS AS (expression) STORED
```

### Performance Configuration Quick Reference

```ini
# Essential PostgreSQL 18 settings for Second Brain
io_method = 'worker'
io_workers = 4
effective_io_concurrency = 200
max_parallel_workers_per_gather = 4
max_parallel_maintenance_workers = 4
shared_preload_libraries = 'pg_stat_statements'
```

### pgvector 0.8 Quick Reference

```sql
-- HNSW index with tuned parameters
CREATE INDEX ON table USING hnsw (embedding vector_cosine_ops)
WITH (m = 24, ef_construction = 128);

-- Search-time tuning
SET hnsw.ef_search = 100;

-- Cosine distance
embedding <=> query_vector

-- Inner product (for normalized vectors)
embedding <#> query_vector

-- L2 distance
embedding <-> query_vector
```

---

## Appendix B: Troubleshooting

### Common Issues

#### Issue: UUIDv7 function not found

```sql
-- Verify PostgreSQL version
SELECT version();
-- Must be 18.0 or higher
```

#### Issue: Async I/O not working

```sql
-- Check if io_method is set correctly
SHOW io_method;
-- Should be 'worker' or 'io_uring'

-- Check for I/O worker processes
SELECT * FROM pg_stat_activity WHERE backend_type = 'io worker';
```

#### Issue: HNSW index build fails

```sql
-- Ensure enough memory
SHOW maintenance_work_mem;
-- Increase if needed
SET maintenance_work_mem = '1GB';
```

#### Issue: Full-text search returns no results

```sql
-- Verify search_vector is populated
SELECT COUNT(*) FROM note_embeddings WHERE search_vector IS NULL;

-- Repopulate if needed
UPDATE note_embeddings
SET search_vector = setweight(to_tsvector('english', COALESCE(note_title, '')), 'A') ||
                   setweight(to_tsvector('english', COALESCE(content, '')), 'B')
WHERE search_vector IS NULL;
```

---

## Document History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2025-12-05 | AI Assistant | Initial comprehensive guide |

---

## References

- [PostgreSQL 18 Release Notes](https://www.postgresql.org/docs/18/release-18.html)
- [pgvector 0.8.0 Release](https://github.com/pgvector/pgvector/releases/tag/v0.8.0)
- [Npgsql 10.0 Documentation](https://www.npgsql.org/efcore/release-notes/10.0.html)
- [PostgreSQL Async I/O Documentation](https://www.postgresql.org/about/featurematrix/detail/asynchronous-io-aio/)
- [UUIDv7 RFC 9562](https://www.rfc-editor.org/rfc/rfc9562)
